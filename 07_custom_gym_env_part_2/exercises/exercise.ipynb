{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1478bea6-bed3-4a05-bdbe-27ce4e0f5f9d",
   "metadata": {},
   "source": [
    "# Inventory management with penalty for unfulfilled demand\n",
    "\n",
    "## Introduction\n",
    "\n",
    "<img src=\"images/2.png\" width=\"500\"/>\n",
    "\n",
    "If a customer comes but your inventory is empty, you have to send the customer away empty-handed (unfulfilled demand). It's a bad experience for the customer, and every time this happens, the customer is less likely to come back to your shop and more likely to go to a competitor. Basically, unfulfilled demand leads to a loss of customer goodwill, and that leads to loss of potential profits.\n",
    "\n",
    "We do not account for this in the course lessons. But in the course exercises, you are tasked to solve the harder (and more realistic) problem that takes this phenomenon into account. That way you can get your hands dirty and practice what you are learning. Are you up for the challenge?\n",
    "\n",
    "\n",
    "## Modeling goodwill loss due to unfulfilled demand\n",
    "\n",
    "Goodwill loss can be best modeled at the level of individual customers. But since our model is at the level of aggregated demand (and not individual customers), we will take a simple approach of punishing the agent each time demand is unfulfilled. This means adding the following term to the reward function.\n",
    "\n",
    "$$-k \\max(0, d - I)$$.\n",
    "\n",
    "Here, $d$ is the realized demand for the day (sampled from a Poisson distribution), $I$ is the on-hand-inventory, and $k$ is the punishment for unit unfulfilled demand. We will call $k$ `goodwill_penalty_per_unit` in our code.\n",
    "\n",
    "## State\n",
    "\n",
    "We want to solve the problem for a defined range of the `goodwill_penalty_per_unit` parameter. \n",
    "\n",
    "$$ 0 \\le \\mathrm{goodwill\\_penalty\\_per\\_unit} \\le 10 $$\n",
    "\n",
    "In each simulation, we will use a different value of this parameter, choosing randomly. So the `goodwill_penalty_per_unit` needs to be a part of our state. The modified state is going to look as follows.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "    s_t & = [ \\mathrm{on\\_hand\\_inventory}_{t}, a_{t-4}, a_{t-3}, a_{t-2}, a_{t-1}, \\lambda, \\\\\n",
    "        & \\phantom{ = [ } \\mathrm{unit\\_selling\\_price}, \\mathrm{unit\\_buying\\_price}, \\\\\n",
    "        & \\phantom{ = [ } \\mathrm{daily\\_unit\\_holding\\_price}, \\mathrm{goodwill\\_penalty\\_per\\_unit}]\n",
    "\\end{aligned}\n",
    "$$.\n",
    "\n",
    "Here we have added `goodwill_penalty_per_unit` at the end of the array.\n",
    "\n",
    "The max value for `goodwill_penalty_per_unit` that we will consider is `10`. This is necessary for defining the high of the observation space.\n",
    "\n",
    "## Your task\n",
    "\n",
    "In the following code block, I have defined the `InventoryEnvHard` class for simulating the inventory management problem with goodwill penalty for unfulilled demand. In the `__init__()` method, I have copied the code from the video lesson. This code corresponds to `InventoryEnv` i.e. without the goodwill penalty. Edit the `__init__()` method to account for the state definition in the harder problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85bee5-2005-4322-85d9-02f7c0c99951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class InventoryEnvHard(gym.Env):\n",
    "    # Edit the __init__() method to match the state definition in the harder problem\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Must define self.observation_space and self.action_space here\n",
    "        \"\"\"\n",
    "\n",
    "        self.max_capacity = 4000\n",
    "        \n",
    "        self.action_space = Box(low=np.array([0]), high=np.array([self.max_capacity]))\n",
    "        \n",
    "        self.lead_time = 5\n",
    "        self.obs_dim = self.lead_time + 4\n",
    "        \n",
    "        self.max_mean_daily_demand = 200\n",
    "        self.max_unit_selling_price = 100\n",
    "        self.max_daily_holding_cost_per_unit = 5\n",
    "        \n",
    "        obs_low = np.zeros((self.obs_dim,))\n",
    "        obs_high = np.array([self.max_capacity for _ in range(self.lead_time)] +\n",
    "                            [self.max_mean_daily_demand, self.max_unit_selling_price,\n",
    "                             self.max_unit_selling_price, self.max_daily_holding_cost_per_unit\n",
    "                             ]\n",
    "                            )\n",
    "        self.observation_space = Box(low=obs_low, high=obs_high)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Returns: the observation of the initial state\n",
    "        Reset the environment to initial state so that a new episode (independent of previous ones) may start\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns: the next observation, the reward, done and optionally additional info\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        Show the current environment state e.g. the graphical window in `CartPole-v1`\n",
    "        This method must be implemented, but it is OK to have an empty implementation if rendering is not\n",
    "        important\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        This method is optional. Used to cleanup all resources (threads, graphical windows) etc.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Returns: List of seeds\n",
    "        This method is optional. Used to set seeds for the environment's random number generator for \n",
    "        obtaining deterministic behavior\n",
    "        \"\"\"\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
