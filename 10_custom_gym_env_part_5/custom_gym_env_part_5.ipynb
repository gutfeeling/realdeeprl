{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d2124f-645c-49fd-9297-4b66967fde48",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a custom `gym` environment for the Inventory Management problem Part 5: Gotchas in `step()` implementation\n",
    "\n",
    "<img src=\"images/shop.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"images/state_action_transition_rewards.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82472759-3422-4d4b-84d6-6415c85915d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "class InventoryEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Must define self.observation_space and self.action_space here\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define action space: bounds, space type, shape\n",
    "        \n",
    "        # Bound: Shelf space is limited\n",
    "        self.max_capacity = 4000\n",
    "        \n",
    "        # Space type: Better to use Box than Discrete, since Discrete will lead to too many output nodes in the NN\n",
    "        # Shape: rllib cannot handle scalar actions, so turn it into a numpy array with shape (1,)\n",
    "        self.action_space = Box(low=np.array([0]), high=np.array([self.max_capacity]))\n",
    "        \n",
    "        # Define observation space: bounds, space type, shape\n",
    "        \n",
    "        # Shape: The lead time controls the shape of observation space\n",
    "        self.lead_time = 5\n",
    "        self.obs_dim = self.lead_time + 4\n",
    "        \n",
    "        # Bounds: Define high of the remaining observation space elements\n",
    "        self.max_mean_daily_demand = 200\n",
    "        self.max_unit_selling_price = 100\n",
    "        self.max_daily_holding_cost_per_unit = 5\n",
    "        \n",
    "        obs_low = np.zeros((self.obs_dim,))\n",
    "        obs_high = np.array([self.max_capacity for _ in range(self.lead_time)] +\n",
    "                            [self.max_mean_daily_demand, self.max_unit_selling_price,\n",
    "                             self.max_unit_selling_price, self.max_daily_holding_cost_per_unit\n",
    "                             ]\n",
    "                            )\n",
    "        self.observation_space = Box(low=obs_low, high=obs_high)\n",
    "        \n",
    "        # The random number generator that will be used throughout the environment\n",
    "        self.rng = default_rng()\n",
    "        \n",
    "        # All instance variables are defined in the __init__() method\n",
    "        self.current_obs = None\n",
    "        self.episode_length_in_days = 90\n",
    "        self.day_num = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Returns: the observation of the initial state\n",
    "        Reset the environment to initial state so that a new episode (independent of previous ones) may start\n",
    "        \"\"\"\n",
    "        # Sample parameter values from the parameter space\n",
    "        \n",
    "        # Set mean daily demand (lambda)\n",
    "        mean_daily_demand = self.rng.uniform() * self.max_mean_daily_demand\n",
    "        \n",
    "        # Set selling price\n",
    "        selling_price = self.rng.uniform() * self.max_unit_selling_price\n",
    "        \n",
    "        # Set buying price: buying price cannot be higher than selling price\n",
    "        buying_price = self.rng.uniform() * selling_price\n",
    "        \n",
    "        # Set daily holding cose per unit: holding cost cannot be higher than buying_price\n",
    "        daily_holding_cost_per_unit = self.rng.uniform() * min(buying_price,\n",
    "                                                               self.max_daily_holding_cost_per_unit\n",
    "                                                               )\n",
    "        \n",
    "        # Return the first observation\n",
    "        self.current_obs = np.array([0 for _ in range(self.lead_time)] +\n",
    "                                    [mean_daily_demand, selling_price, buying_price,\n",
    "                                     daily_holding_cost_per_unit,\n",
    "                                     ]\n",
    "                                    )\n",
    "        self.day_num = 0\n",
    "        return self.current_obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns: Given current obs and action, returns the next observation, the reward, done and optionally additional info\n",
    "        \"\"\"\n",
    "        # Action looks like np.array([20.0]). We convert that to float 20.0 for easier calculation\n",
    "        buys = min(action[0], self.max_capacity - np.sum(self.current_obs[:self.lead_time]))\n",
    "        \n",
    "        # Compute next obs\n",
    "        demand = self.rng.poisson(self.current_obs[self.lead_time])\n",
    "        next_obs = np.concatenate((self.current_obs[1: self.lead_time],\n",
    "                                   np.array([buys]),\n",
    "                                   self.current_obs[self.lead_time:]\n",
    "                                   )\n",
    "                                  )\n",
    "        next_obs[0] += max(0, self.current_obs[0] - demand)\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = (self.current_obs[self.lead_time + 1] * (self.current_obs[0] + self.current_obs[1] - next_obs[0]) -\n",
    "                  self.current_obs[self.lead_time + 2] * buys - \n",
    "                  self.current_obs[self.lead_time + 3] * (next_obs[0] - self.current_obs[1])\n",
    "                  )\n",
    "                  \n",
    "        # Compute done\n",
    "        self.day_num += 1\n",
    "        done = False\n",
    "        if self.day_num >= self.episode_length_in_days:\n",
    "            done = True\n",
    "            \n",
    "        self.current_obs = next_obs\n",
    "\n",
    "        # info must be a dict\n",
    "        return self.current_obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        Show the current environment state e.g. the graphical window in `CartPole-v1`\n",
    "        This method must be implemented, but it is OK to have an empty implementation if rendering is not\n",
    "        important\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        This method is optional. Used to cleanup all resources (threads, graphical windows) etc.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Returns: List of seeds\n",
    "        This method is optional. Used to set seeds for the environment's random number generator for \n",
    "        obtaining deterministic behavior\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cae5cc4-4701-4d84-8e33-047e8eaad2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dibya/miniconda3/envs/real_world_deep_rl_course_cpu/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Observation [5.99800000e+03 3.00000000e+03 3.00000000e+03 3.00000000e+03\n 3.00000000e+03 6.94954109e-01 4.41213796e+00 4.10669941e+00\n 4.00165833e-01] does not respect observation space upper bound Box([0. 0. 0. 0. 0. 0. 0. 0. 0.], [4000. 4000. 4000. 4000. 4000.  200.  100.  100.    5.], (9,), float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m3000\u001b[39m])\n\u001b[1;32m      6\u001b[0m obs, r, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(obs \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mhigh), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not respect observation space upper bound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Observation [5.99800000e+03 3.00000000e+03 3.00000000e+03 3.00000000e+03\n 3.00000000e+03 6.94954109e-01 4.41213796e+00 4.10669941e+00\n 4.00165833e-01] does not respect observation space upper bound Box([0. 0. 0. 0. 0. 0. 0. 0. 0.], [4000. 4000. 4000. 4000. 4000.  200.  100.  100.    5.], (9,), float32)"
     ]
    }
   ],
   "source": [
    "env = InventoryEnv()\n",
    "for _ in range(1000):\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action = np.array([3000])\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        assert np.all(obs <= env.observation_space.high), f\"Observation {obs} does not respect observation space upper bound {env.observation_space}\"\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2aefa-fdeb-4d5f-afc4-914c0e767145",
   "metadata": {},
   "source": [
    "| State | Day num | Buys | Demand |\n",
    "| --- | --- | --- | --- |\n",
    "| `[0, 0, 0, 0, 0, ...]` | 0 | 3000 | 0 |\n",
    "| `[0, 0, 0, 0, 3000, ...]` | 1 | 3000 | 0 |\n",
    "| `[0, 0, 0, 3000, 3000, ...]` | 2 | 3000 | 0 |\n",
    "| `[0, 0, 3000, 3000, 3000, ...]` | 3 | 3000 | 0 |\n",
    "| `[0, 3000, 3000, 3000, 3000, ...]` | 4 | 3000 | 0 |\n",
    "| `[3000, 3000, 3000, 3000, 3000, ...]` | 5 | 3000 | 0 |\n",
    "| `[6000, 3000, 3000, 3000, 3000, ...]` | 6 | 3000 | 0 |\n",
    "\n",
    "\n",
    "- All units that are already in the inventory or will eventually arrive in the shop:  `self.current_obs[:self.lead_time]`.\n",
    "- Remaining inventory space in case of `0` demand:  `self.max_capacity - self.current_obs[:self.lead_time]`.\n",
    "- `buys = min(action[0], self.max_capacity - self.current_obs[:self.lead_time])`.\n",
    "\n",
    "| State | Day num | Buys | Buys after imposing constraint | Demand |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| `[0, 0, 0, 0, 0, ...]` | 0 | 3000 | 3000 | 0 |\n",
    "| `[0, 0, 0, 0, 3000, ...]` | 1 | 3000 | 1000 | 0 |\n",
    "| `[0, 0, 0, 3000, 1000, ...]` | 2 | 3000 | 0 | 0 |\n",
    "| `[0, 0, 3000, 1000, 0, ...]` | 3 | 3000 | 0 | 0 |\n",
    "| `[0, 3000, 1000, 0, 0, ...]` | 4 | 3000 | 0 | 0 |\n",
    "| `[3000, 1000, 0, 0, 0, ...]` | 5 | 3000 | 0 | 0 |\n",
    "| `[4000, 0, 0, 0, 0, ...]` | 6 | 3000 | 0 | 0 |\n",
    "\n",
    "- **action postprocessing due to constraint**: can affect performance significantly, because the agent thinks it is performing a particular action, but a different action is actually being executed in the environment. It may be hard for the agent to learn this connection.\n",
    "\n",
    "- Try different ways of imposing the constraint to check if they lead to different performances.\n",
    "\n",
    "- Test your environment thoroughly (especially if action and environment bounds are being respected), and if possible, by writing unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045f9e7-abac-47f6-8d31-0b7da147adf0",
   "metadata": {},
   "source": [
    "<img src=\"images/inv_sim.png\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d569e2-934f-4e64-9ad4-87569771c178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
