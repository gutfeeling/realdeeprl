{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6b69f7-c4fa-428d-9b13-8ee3a387d568",
   "metadata": {},
   "source": [
    "# Using `rllib` to solve the inventory management custom environment\n",
    "\n",
    "<img src=\"images/inv_sim.png\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419cade2-fca7-4ca3-b72a-f92624e6b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "class InventoryEnv(gym.Env):\n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"\n",
    "        Must define self.observation_space and self.action_space here\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define action space: bounds, space type, shape\n",
    "        \n",
    "        # Bound: Shelf space is limited\n",
    "        self.max_capacity = 4000\n",
    "        \n",
    "        # Space type: Better to use Box than Discrete, since Discrete will lead to too many output nodes in the NN\n",
    "        # Shape: rllib cannot handle scalar actions, so turn it into a numpy array with shape (1,)\n",
    "        self.action_space = Box(low=np.array([0]), high=np.array([self.max_capacity]))\n",
    "        \n",
    "        # Define observation space: bounds, space type, shape\n",
    "        \n",
    "        # Shape: The lead time controls the shape of observation space\n",
    "        self.lead_time = 5\n",
    "        self.obs_dim = self.lead_time + 4\n",
    "        \n",
    "        # Bounds: Define high of the remaining observation space elements\n",
    "        self.max_mean_daily_demand = 200\n",
    "        self.max_unit_selling_price = 100\n",
    "        self.max_daily_holding_cost_per_unit = 5\n",
    "        \n",
    "        obs_low = np.zeros((self.obs_dim,))\n",
    "        obs_high = np.array([self.max_capacity for _ in range(self.lead_time)] +\n",
    "                            [self.max_mean_daily_demand, self.max_unit_selling_price,\n",
    "                             self.max_unit_selling_price, self.max_daily_holding_cost_per_unit\n",
    "                             ]\n",
    "                            )\n",
    "        self.observation_space = Box(low=obs_low, high=obs_high)\n",
    "        \n",
    "        # The random number generator that will be used throughout the environment\n",
    "        self.rng = default_rng()\n",
    "        \n",
    "        # All instance variables are defined in the __init__() method\n",
    "        self.current_obs = None\n",
    "        self.episode_length_in_days = 90\n",
    "        self.day_num = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Returns: the observation of the initial state\n",
    "        Reset the environment to initial state so that a new episode (independent of previous ones) may start\n",
    "        \"\"\"\n",
    "        # Sample parameter values from the parameter space\n",
    "        \n",
    "        # Set mean daily demand (lambda)\n",
    "        mean_daily_demand = self.rng.uniform() * self.max_mean_daily_demand\n",
    "        \n",
    "        # Set selling price\n",
    "        selling_price = self.rng.uniform() * self.max_unit_selling_price\n",
    "        \n",
    "        # Set buying price: buying price cannot be higher than selling price\n",
    "        buying_price = self.rng.uniform() * selling_price\n",
    "        \n",
    "        # Set daily holding cose per unit: holding cost cannot be higher than buying_price\n",
    "        daily_holding_cost_per_unit = self.rng.uniform() * min(buying_price,\n",
    "                                                               self.max_daily_holding_cost_per_unit\n",
    "                                                               )\n",
    "        \n",
    "        # Return the first observation\n",
    "        self.current_obs = np.array([0 for _ in range(self.lead_time)] +\n",
    "                                    [mean_daily_demand, selling_price, buying_price,\n",
    "                                     daily_holding_cost_per_unit,\n",
    "                                     ]\n",
    "                                    )\n",
    "        self.day_num = 0\n",
    "        return self.current_obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns: Given current obs and action, returns the next observation, the reward, done and optionally additional info\n",
    "        \"\"\"\n",
    "        # Action looks like np.array([20.0]). We convert that to float 20.0 for easier calculation\n",
    "        buys = min(action[0], self.max_capacity - np.sum(self.current_obs[:self.lead_time]))\n",
    "        \n",
    "        # Compute next obs\n",
    "        demand = self.rng.poisson(self.current_obs[self.lead_time])\n",
    "        next_obs = np.concatenate((self.current_obs[1: self.lead_time],\n",
    "                                   np.array([buys]),\n",
    "                                   self.current_obs[self.lead_time:]\n",
    "                                   )\n",
    "                                  )\n",
    "        next_obs[0] += max(0, self.current_obs[0] - demand)\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = (self.current_obs[self.lead_time + 1] * (self.current_obs[0] + self.current_obs[1] - next_obs[0]) -\n",
    "                  self.current_obs[self.lead_time + 2] * buys - \n",
    "                  self.current_obs[self.lead_time + 3] * (next_obs[0] - self.current_obs[1])\n",
    "                  )\n",
    "                  \n",
    "        # Compute done\n",
    "        self.day_num += 1\n",
    "        done = False\n",
    "        if self.day_num >= self.episode_length_in_days:\n",
    "            done = True\n",
    "            \n",
    "        self.current_obs = next_obs\n",
    "\n",
    "        # info must be a dict\n",
    "        return self.current_obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        Show the current environment state e.g. the graphical window in `CartPole-v1`\n",
    "        This method must be implemented, but it is OK to have an empty implementation if rendering is not\n",
    "        important\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        This method is optional. Used to cleanup all resources (threads, graphical windows) etc.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Returns: List of seeds\n",
    "        This method is optional. Used to set seeds for the environment's random number generator for \n",
    "        obtaining deterministic behavior\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e8a86-b8e4-414f-9afc-d989943cb4af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:09,521\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:09,522\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:09,522\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:09,522\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27801)\u001b[0m /home/dibya/miniconda3/envs/real_world_deep_rl_course_cpu/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27801)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27792)\u001b[0m /home/dibya/miniconda3/envs/real_world_deep_rl_course_cpu/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27792)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:12,857\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m /home/dibya/miniconda3/envs/real_world_deep_rl_course_cpu/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:13,509\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:13 (running for 00:00:06.00)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:14 (running for 00:00:07.01)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:16,755\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:19 (running for 00:00:12.03)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:20,502\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 45103.0x the scale of `vf_clip_param`. This means that it will take more than 45103.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-14-20\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 636241.2473832322\n",
      "  episode_reward_mean: -451034.21460738644\n",
      "  episode_reward_min: -1438715.5781107056\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 44\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4288417100906372\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0028741948772221804\n",
      "          model: {}\n",
      "          policy_loss: -0.0014492435147985816\n",
      "          total_loss: 52274954240.0\n",
      "          vf_explained_var: -6.647520081060065e-07\n",
      "          vf_loss: 52274954240.0\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.290000000000001\n",
      "    ram_util_percent: 17.35\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18151809667599672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08976012691743727\n",
      "    mean_inference_ms: 1.1269648274083781\n",
      "    mean_raw_obs_processing_ms: 0.13507175064277555\n",
      "  time_since_restore: 6.987516164779663\n",
      "  time_this_iter_s: 6.987516164779663\n",
      "  time_total_s: 6.987516164779663\n",
      "  timers:\n",
      "    learn_throughput: 1073.851\n",
      "    learn_time_ms: 3724.911\n",
      "    load_throughput: 10944041.748\n",
      "    load_time_ms: 0.365\n",
      "    sample_throughput: 1026.277\n",
      "    sample_time_ms: 3897.583\n",
      "    update_time_ms: 2.765\n",
      "  timestamp: 1665666860\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:25 (running for 00:00:18.03)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.98752</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -451034</td><td style=\"text-align: right;\">              636241</td><td style=\"text-align: right;\">        -1.43872e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-14-27\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 636241.2473832322\n",
      "  episode_reward_mean: -454837.1052217662\n",
      "  episode_reward_min: -1535093.629592125\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 88\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4052077531814575\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006940712686628103\n",
      "          model: {}\n",
      "          policy_loss: -0.008040248416364193\n",
      "          total_loss: 69405171712.0\n",
      "          vf_explained_var: -3.22634178928638e-07\n",
      "          vf_loss: 69405171712.0\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.74\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1819363822022023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09002303632432028\n",
      "    mean_inference_ms: 1.1212310373564576\n",
      "    mean_raw_obs_processing_ms: 0.1349584118962971\n",
      "  time_since_restore: 13.640584468841553\n",
      "  time_this_iter_s: 6.65306830406189\n",
      "  time_total_s: 13.640584468841553\n",
      "  timers:\n",
      "    learn_throughput: 1115.633\n",
      "    learn_time_ms: 3585.409\n",
      "    load_throughput: 12975418.407\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 734.512\n",
      "    sample_time_ms: 5445.794\n",
      "    update_time_ms: 2.91\n",
      "  timestamp: 1665666867\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:27,206\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 45484.0x the scale of `vf_clip_param`. This means that it will take more than 45484.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:31 (running for 00:00:23.78)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         13.6406</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -454837</td><td style=\"text-align: right;\">              636241</td><td style=\"text-align: right;\">        -1.53509e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-14-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 520561.9104830079\n",
      "  episode_reward_mean: -507707.9893344553\n",
      "  episode_reward_min: -1535093.629592125\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 132\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3370603322982788\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008609519340097904\n",
      "          model: {}\n",
      "          policy_loss: -0.012667425908148289\n",
      "          total_loss: 69572132864.0\n",
      "          vf_explained_var: -8.466422940500706e-08\n",
      "          vf_loss: 69572132864.0\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.290000000000001\n",
      "    ram_util_percent: 17.330000000000002\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1835522959557013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09080579215257117\n",
      "    mean_inference_ms: 1.1192296962671247\n",
      "    mean_raw_obs_processing_ms: 0.1357227301613216\n",
      "  time_since_restore: 20.411640405654907\n",
      "  time_this_iter_s: 6.7710559368133545\n",
      "  time_total_s: 20.411640405654907\n",
      "  timers:\n",
      "    learn_throughput: 1132.208\n",
      "    learn_time_ms: 3532.92\n",
      "    load_throughput: 12467586.822\n",
      "    load_time_ms: 0.321\n",
      "    sample_throughput: 676.743\n",
      "    sample_time_ms: 5910.666\n",
      "    update_time_ms: 2.726\n",
      "  timestamp: 1665666874\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:34,028\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 50771.0x the scale of `vf_clip_param`. This means that it will take more than 50771.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:37 (running for 00:00:29.56)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         20.4116</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -507708</td><td style=\"text-align: right;\">              520562</td><td style=\"text-align: right;\">        -1.53509e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-14-40\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 739623.9444485448\n",
      "  episode_reward_mean: -505462.6560601601\n",
      "  episode_reward_min: -1722478.8408258236\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 176\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.330762267112732\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005340712610632181\n",
      "          model: {}\n",
      "          policy_loss: -0.002432489302009344\n",
      "          total_loss: 63003963392.0\n",
      "          vf_explained_var: -2.7366864330247154e-08\n",
      "          vf_loss: 63003963392.0\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.933333333333332\n",
      "    ram_util_percent: 17.3\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18418545719930912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09117425540013542\n",
      "    mean_inference_ms: 1.1146731102321608\n",
      "    mean_raw_obs_processing_ms: 0.1361881665457611\n",
      "  time_since_restore: 26.98121166229248\n",
      "  time_this_iter_s: 6.569571256637573\n",
      "  time_total_s: 26.98121166229248\n",
      "  timers:\n",
      "    learn_throughput: 1141.52\n",
      "    learn_time_ms: 3504.101\n",
      "    load_throughput: 14582543.242\n",
      "    load_time_ms: 0.274\n",
      "    sample_throughput: 656.679\n",
      "    sample_time_ms: 6091.255\n",
      "    update_time_ms: 2.718\n",
      "  timestamp: 1665666880\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:40,651\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 50546.0x the scale of `vf_clip_param`. This means that it will take more than 50546.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:42 (running for 00:00:35.18)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         26.9812</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -505463</td><td style=\"text-align: right;\">              739624</td><td style=\"text-align: right;\">        -1.72248e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:47,400\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 54041.0x the scale of `vf_clip_param`. This means that it will take more than 54041.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-14-47\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 739623.9444485448\n",
      "  episode_reward_mean: -540412.8526597274\n",
      "  episode_reward_min: -1787960.033802799\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 222\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2272062301635742\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010175843723118305\n",
      "          model: {}\n",
      "          policy_loss: -0.0010648787720128894\n",
      "          total_loss: 93502005248.0\n",
      "          vf_explained_var: -2.5059586405973278e-08\n",
      "          vf_loss: 93502005248.0\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.59\n",
      "    ram_util_percent: 17.300000000000004\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18418988489539234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0912082816701168\n",
      "    mean_inference_ms: 1.1110855122182128\n",
      "    mean_raw_obs_processing_ms: 0.1361757631001436\n",
      "  time_since_restore: 33.68255925178528\n",
      "  time_this_iter_s: 6.701347589492798\n",
      "  time_total_s: 33.68255925178528\n",
      "  timers:\n",
      "    learn_throughput: 1146.818\n",
      "    learn_time_ms: 3487.913\n",
      "    load_throughput: 15406075.298\n",
      "    load_time_ms: 0.26\n",
      "    sample_throughput: 642.913\n",
      "    sample_time_ms: 6221.685\n",
      "    update_time_ms: 2.765\n",
      "  timestamp: 1665666887\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:48 (running for 00:00:40.93)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         33.6826</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -540413</td><td style=\"text-align: right;\">              739624</td><td style=\"text-align: right;\">        -1.78796e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:53 (running for 00:00:45.98)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         33.6826</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -540413</td><td style=\"text-align: right;\">              739624</td><td style=\"text-align: right;\">        -1.78796e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:14:54,174\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 53047.0x the scale of `vf_clip_param`. This means that it will take more than 53047.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-14-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 723253.3458310947\n",
      "  episode_reward_mean: -530472.738902019\n",
      "  episode_reward_min: -1787960.033802799\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 266\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1995543241500854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01192067377269268\n",
      "          model: {}\n",
      "          policy_loss: -0.011487104929983616\n",
      "          total_loss: 54479822848.0\n",
      "          vf_explained_var: -1.0318653487217944e-08\n",
      "          vf_loss: 54479822848.0\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.477777777777778\n",
      "    ram_util_percent: 17.3\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1843797787731284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.091331939917741\n",
      "    mean_inference_ms: 1.110946001803466\n",
      "    mean_raw_obs_processing_ms: 0.13630310454855094\n",
      "  time_since_restore: 40.40418839454651\n",
      "  time_this_iter_s: 6.7216291427612305\n",
      "  time_total_s: 40.40418839454651\n",
      "  timers:\n",
      "    learn_throughput: 1150.655\n",
      "    learn_time_ms: 3476.282\n",
      "    load_throughput: 14588883.478\n",
      "    load_time_ms: 0.274\n",
      "    sample_throughput: 633.456\n",
      "    sample_time_ms: 6314.567\n",
      "    update_time_ms: 2.768\n",
      "  timestamp: 1665666894\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:14:59 (running for 00:00:51.71)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         40.4042</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -530473</td><td style=\"text-align: right;\">              723253</td><td style=\"text-align: right;\">        -1.78796e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 723253.3458310947\n",
      "  episode_reward_mean: -506115.1150541123\n",
      "  episode_reward_min: -1599703.4826916477\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 310\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2147294282913208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010887835174798965\n",
      "          model: {}\n",
      "          policy_loss: -0.006326260045170784\n",
      "          total_loss: 70321250304.0\n",
      "          vf_explained_var: -2.204730975563507e-08\n",
      "          vf_loss: 70321250304.0\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.719999999999999\n",
      "    ram_util_percent: 17.300000000000004\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18425686245352124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09126805132335153\n",
      "    mean_inference_ms: 1.109225837431174\n",
      "    mean_raw_obs_processing_ms: 0.1361528976992997\n",
      "  time_since_restore: 47.030274868011475\n",
      "  time_this_iter_s: 6.626086473464966\n",
      "  time_total_s: 47.030274868011475\n",
      "  timers:\n",
      "    learn_throughput: 1153.436\n",
      "    learn_time_ms: 3467.9\n",
      "    load_throughput: 13977685.313\n",
      "    load_time_ms: 0.286\n",
      "    sample_throughput: 628.226\n",
      "    sample_time_ms: 6367.134\n",
      "    update_time_ms: 2.739\n",
      "  timestamp: 1665666900\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:15:00,855\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 50612.0x the scale of `vf_clip_param`. This means that it will take more than 50612.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:04 (running for 00:00:57.41)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         47.0303</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -506115</td><td style=\"text-align: right;\">              723253</td><td style=\"text-align: right;\">         -1.5997e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 992515.6231428592\n",
      "  episode_reward_mean: -484195.20645574294\n",
      "  episode_reward_min: -1646359.783836148\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 354\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2212870121002197\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016632264479994774\n",
      "          model: {}\n",
      "          policy_loss: -0.012561638839542866\n",
      "          total_loss: 74371817472.0\n",
      "          vf_explained_var: -1.1087745832583096e-08\n",
      "          vf_loss: 74371817472.0\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.080000000000002\n",
      "    ram_util_percent: 17.340000000000003\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18442461113243433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09134111070175059\n",
      "    mean_inference_ms: 1.109480295217847\n",
      "    mean_raw_obs_processing_ms: 0.1362256632960045\n",
      "  time_since_restore: 53.764304399490356\n",
      "  time_this_iter_s: 6.734029531478882\n",
      "  time_total_s: 53.764304399490356\n",
      "  timers:\n",
      "    learn_throughput: 1155.214\n",
      "    learn_time_ms: 3462.561\n",
      "    load_throughput: 13634470.54\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 623.321\n",
      "    sample_time_ms: 6417.24\n",
      "    update_time_ms: 2.713\n",
      "  timestamp: 1665666907\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:15:07,631\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 48420.0x the scale of `vf_clip_param`. This means that it will take more than 48420.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:10 (running for 00:01:03.15)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         53.7643</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -484195</td><td style=\"text-align: right;\">              992516</td><td style=\"text-align: right;\">        -1.64636e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-15-14\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 992515.6231428592\n",
      "  episode_reward_mean: -506491.8513896533\n",
      "  episode_reward_min: -1646359.783836148\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 400\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1226975917816162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01144337560981512\n",
      "          model: {}\n",
      "          policy_loss: -0.0030552197713404894\n",
      "          total_loss: 66072473600.0\n",
      "          vf_explained_var: -2.0957761748263692e-08\n",
      "          vf_loss: 66072473600.0\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.955555555555556\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1844225845250972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09132790374674342\n",
      "    mean_inference_ms: 1.108992974287464\n",
      "    mean_raw_obs_processing_ms: 0.13626035706066933\n",
      "  time_since_restore: 60.43524527549744\n",
      "  time_this_iter_s: 6.67094087600708\n",
      "  time_total_s: 60.43524527549744\n",
      "  timers:\n",
      "    learn_throughput: 1154.058\n",
      "    learn_time_ms: 3466.032\n",
      "    load_throughput: 13342311.92\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 620.912\n",
      "    sample_time_ms: 6442.14\n",
      "    update_time_ms: 2.721\n",
      "  timestamp: 1665666914\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:15:14,342\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 50649.0x the scale of `vf_clip_param`. This means that it will take more than 50649.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:16 (running for 00:01:08.87)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         60.4352</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -506492</td><td style=\"text-align: right;\">              992516</td><td style=\"text-align: right;\">        -1.64636e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-15-20\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1048078.7521710768\n",
      "  episode_reward_mean: -471272.3734028735\n",
      "  episode_reward_min: -1720230.431572242\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 444\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.091694712638855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01276425551623106\n",
      "          model: {}\n",
      "          policy_loss: -0.008744750171899796\n",
      "          total_loss: 74299408384.0\n",
      "          vf_explained_var: -3.140459670092355e-09\n",
      "          vf_loss: 74299408384.0\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1843387886624869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09124108625718709\n",
      "    mean_inference_ms: 1.1074941655653419\n",
      "    mean_raw_obs_processing_ms: 0.1362376972277606\n",
      "  time_since_restore: 67.03528213500977\n",
      "  time_this_iter_s: 6.600036859512329\n",
      "  time_total_s: 67.03528213500977\n",
      "  timers:\n",
      "    learn_throughput: 1155.696\n",
      "    learn_time_ms: 3461.117\n",
      "    load_throughput: 13146227.864\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 618.148\n",
      "    sample_time_ms: 6470.946\n",
      "    update_time_ms: 2.7\n",
      "  timestamp: 1665666920\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:15:20,995\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 47127.0x the scale of `vf_clip_param`. This means that it will take more than 47127.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:22 (running for 00:01:14.56)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         67.0353</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -471272</td><td style=\"text-align: right;\">         1.04808e+06</td><td style=\"text-align: right;\">        -1.72023e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:27 (running for 00:01:19.56)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         67.0353</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -471272</td><td style=\"text-align: right;\">         1.04808e+06</td><td style=\"text-align: right;\">        -1.72023e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-15-27\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1048078.7521710768\n",
      "  episode_reward_mean: -474339.4308785737\n",
      "  episode_reward_min: -1720230.431572242\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 488\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1692538261413574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012427011504769325\n",
      "          model: {}\n",
      "          policy_loss: -0.007687926758080721\n",
      "          total_loss: 57269559296.0\n",
      "          vf_explained_var: -2.6213225368110216e-08\n",
      "          vf_loss: 57269559296.0\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.511111111111111\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1839863302913148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09100804888488666\n",
      "    mean_inference_ms: 1.1043188634770025\n",
      "    mean_raw_obs_processing_ms: 0.13597323284317903\n",
      "  time_since_restore: 73.60183024406433\n",
      "  time_this_iter_s: 6.566548109054565\n",
      "  time_total_s: 73.60183024406433\n",
      "  timers:\n",
      "    learn_throughput: 1165.171\n",
      "    learn_time_ms: 3432.974\n",
      "    load_throughput: 13543119.148\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 593.405\n",
      "    sample_time_ms: 6740.756\n",
      "    update_time_ms: 2.689\n",
      "  timestamp: 1665666927\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:15:27,616\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 47434.0x the scale of `vf_clip_param`. This means that it will take more than 47434.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:32 (running for 00:01:25.18)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         73.6018</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -474339</td><td style=\"text-align: right;\">         1.04808e+06</td><td style=\"text-align: right;\">        -1.72023e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:15:34,222\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 47632.0x the scale of `vf_clip_param`. This means that it will take more than 47632.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-15-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 593284.9619669828\n",
      "  episode_reward_mean: -476320.4543224036\n",
      "  episode_reward_min: -1548356.8467520196\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 532\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2163108587265015\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007108486723154783\n",
      "          model: {}\n",
      "          policy_loss: -0.00836988165974617\n",
      "          total_loss: 68720615424.0\n",
      "          vf_explained_var: -5.127281266226191e-09\n",
      "          vf_loss: 68720615424.0\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.99\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1833773520426089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09068937629847643\n",
      "    mean_inference_ms: 1.1001115292797268\n",
      "    mean_raw_obs_processing_ms: 0.13550113834196562\n",
      "  time_since_restore: 80.1555564403534\n",
      "  time_this_iter_s: 6.5537261962890625\n",
      "  time_total_s: 80.1555564403534\n",
      "  timers:\n",
      "    learn_throughput: 1165.668\n",
      "    learn_time_ms: 3431.507\n",
      "    load_throughput: 12944383.921\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 596.742\n",
      "    sample_time_ms: 6703.067\n",
      "    update_time_ms: 2.647\n",
      "  timestamp: 1665666934\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:38 (running for 00:01:30.74)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         80.1556</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -476320</td><td style=\"text-align: right;\">              593285</td><td style=\"text-align: right;\">        -1.54836e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-15-40\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 708009.2586920443\n",
      "  episode_reward_mean: -481099.7744528146\n",
      "  episode_reward_min: -1548356.8467520196\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 576\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.229792594909668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010520200245082378\n",
      "          model: {}\n",
      "          policy_loss: -0.012441177852451801\n",
      "          total_loss: 63553712128.0\n",
      "          vf_explained_var: 2.4354585015373686e-09\n",
      "          vf_loss: 63553712128.0\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.488888888888889\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18301784699595175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09050130977894755\n",
      "    mean_inference_ms: 1.0972792130595035\n",
      "    mean_raw_obs_processing_ms: 0.13518478553365207\n",
      "  time_since_restore: 86.6995689868927\n",
      "  time_this_iter_s: 6.544012546539307\n",
      "  time_total_s: 86.6995689868927\n",
      "  timers:\n",
      "    learn_throughput: 1165.12\n",
      "    learn_time_ms: 3433.124\n",
      "    load_throughput: 12923444.77\n",
      "    load_time_ms: 0.31\n",
      "    sample_throughput: 599.147\n",
      "    sample_time_ms: 6676.163\n",
      "    update_time_ms: 2.625\n",
      "  timestamp: 1665666940\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:15:40,805\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 48110.0x the scale of `vf_clip_param`. This means that it will take more than 48110.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:43 (running for 00:01:36.37)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         86.6996</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -481100</td><td style=\"text-align: right;\">              708009</td><td style=\"text-align: right;\">        -1.54836e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:15:47,477\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 51014.0x the scale of `vf_clip_param`. This means that it will take more than 51014.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-15-47\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 708009.2586920443\n",
      "  episode_reward_mean: -510137.397576981\n",
      "  episode_reward_min: -1877339.6774211489\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 622\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1002966165542603\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013408933766186237\n",
      "          model: {}\n",
      "          policy_loss: -0.004241900518536568\n",
      "          total_loss: 86448431104.0\n",
      "          vf_explained_var: -4.403052855650458e-08\n",
      "          vf_loss: 86448431104.0\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.599999999999998\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18278838491350066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09038371478317357\n",
      "    mean_inference_ms: 1.0957455427360636\n",
      "    mean_raw_obs_processing_ms: 0.1349933921058478\n",
      "  time_since_restore: 93.32451558113098\n",
      "  time_this_iter_s: 6.624946594238281\n",
      "  time_total_s: 93.32451558113098\n",
      "  timers:\n",
      "    learn_throughput: 1164.674\n",
      "    learn_time_ms: 3434.436\n",
      "    load_throughput: 12083849.035\n",
      "    load_time_ms: 0.331\n",
      "    sample_throughput: 598.691\n",
      "    sample_time_ms: 6681.245\n",
      "    update_time_ms: 2.665\n",
      "  timestamp: 1665666947\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:49 (running for 00:01:42.01)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         93.3245</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -510137</td><td style=\"text-align: right;\">              708009</td><td style=\"text-align: right;\">        -1.87734e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-15-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1188892.441021654\n",
      "  episode_reward_mean: -559905.745712357\n",
      "  episode_reward_min: -1877339.6774211489\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 666\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.131700873374939\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015114150941371918\n",
      "          model: {}\n",
      "          policy_loss: -0.008312167599797249\n",
      "          total_loss: 87597039616.0\n",
      "          vf_explained_var: 1.2369565816072736e-08\n",
      "          vf_loss: 87597039616.0\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.179999999999998\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18262867025210738\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09028764626810654\n",
      "    mean_inference_ms: 1.0947877806096196\n",
      "    mean_raw_obs_processing_ms: 0.1348938437619641\n",
      "  time_since_restore: 99.89390540122986\n",
      "  time_this_iter_s: 6.569389820098877\n",
      "  time_total_s: 99.89390540122986\n",
      "  timers:\n",
      "    learn_throughput: 1164.556\n",
      "    learn_time_ms: 3434.787\n",
      "    load_throughput: 11584075.123\n",
      "    load_time_ms: 0.345\n",
      "    sample_throughput: 599.731\n",
      "    sample_time_ms: 6669.662\n",
      "    update_time_ms: 2.649\n",
      "  timestamp: 1665666954\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:15:54,100\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 55991.0x the scale of `vf_clip_param`. This means that it will take more than 55991.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:15:55 (running for 00:01:47.63)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         99.8939</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -559906</td><td style=\"text-align: right;\">         1.18889e+06</td><td style=\"text-align: right;\">        -1.87734e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:00 (running for 00:01:52.63)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         99.8939</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -559906</td><td style=\"text-align: right;\">         1.18889e+06</td><td style=\"text-align: right;\">        -1.87734e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:16:00,819\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 61602.0x the scale of `vf_clip_param`. This means that it will take more than 61602.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1188892.441021654\n",
      "  episode_reward_mean: -616016.7736558075\n",
      "  episode_reward_min: -1563228.2229759796\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 710\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0605506896972656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014779823832213879\n",
      "          model: {}\n",
      "          policy_loss: -0.006850025150924921\n",
      "          total_loss: 83409895424.0\n",
      "          vf_explained_var: 2.3072765031884046e-08\n",
      "          vf_loss: 83409895424.0\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.88888888888889\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1826202646898519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09026196357301411\n",
      "    mean_inference_ms: 1.0946959839728563\n",
      "    mean_raw_obs_processing_ms: 0.13491797417772816\n",
      "  time_since_restore: 106.56011986732483\n",
      "  time_this_iter_s: 6.666214466094971\n",
      "  time_total_s: 106.56011986732483\n",
      "  timers:\n",
      "    learn_throughput: 1162.664\n",
      "    learn_time_ms: 3440.376\n",
      "    load_throughput: 12054329.645\n",
      "    load_time_ms: 0.332\n",
      "    sample_throughput: 600.699\n",
      "    sample_time_ms: 6658.909\n",
      "    update_time_ms: 2.624\n",
      "  timestamp: 1665666960\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:05 (running for 00:01:58.39)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">          106.56</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -616017</td><td style=\"text-align: right;\">         1.18889e+06</td><td style=\"text-align: right;\">        -1.56323e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:16:07,592\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 60386.0x the scale of `vf_clip_param`. This means that it will take more than 60386.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-16-07\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1188892.441021654\n",
      "  episode_reward_mean: -603863.791011416\n",
      "  episode_reward_min: -1696006.074940232\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 754\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.299355149269104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019126752391457558\n",
      "          model: {}\n",
      "          policy_loss: -0.01123115699738264\n",
      "          total_loss: 58862710784.0\n",
      "          vf_explained_var: -1.4356387190161968e-08\n",
      "          vf_loss: 58862710784.0\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.940000000000001\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1827351197417522\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09030881756317805\n",
      "    mean_inference_ms: 1.0954028284498754\n",
      "    mean_raw_obs_processing_ms: 0.1349780008849743\n",
      "  time_since_restore: 113.28480815887451\n",
      "  time_this_iter_s: 6.724688291549683\n",
      "  time_total_s: 113.28480815887451\n",
      "  timers:\n",
      "    learn_throughput: 1161.312\n",
      "    learn_time_ms: 3444.38\n",
      "    load_throughput: 12682149.822\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 599.742\n",
      "    sample_time_ms: 6669.537\n",
      "    update_time_ms: 2.642\n",
      "  timestamp: 1665666967\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:11 (running for 00:02:04.12)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         113.285</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -603864</td><td style=\"text-align: right;\">         1.18889e+06</td><td style=\"text-align: right;\">        -1.69601e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-16-14\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 300079.0116544068\n",
      "  episode_reward_mean: -560558.749164979\n",
      "  episode_reward_min: -1796236.4037087257\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 800\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.316943883895874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011346153914928436\n",
      "          model: {}\n",
      "          policy_loss: -0.01066114567220211\n",
      "          total_loss: 77113106432.0\n",
      "          vf_explained_var: -3.653187974350658e-09\n",
      "          vf_loss: 77113106432.0\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.680000000000001\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18293704978131878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09042161692767749\n",
      "    mean_inference_ms: 1.0968134164825207\n",
      "    mean_raw_obs_processing_ms: 0.13506528424609102\n",
      "  time_since_restore: 120.03021383285522\n",
      "  time_this_iter_s: 6.745405673980713\n",
      "  time_total_s: 120.03021383285522\n",
      "  timers:\n",
      "    learn_throughput: 1161.803\n",
      "    learn_time_ms: 3442.925\n",
      "    load_throughput: 13112322.001\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 599.035\n",
      "    sample_time_ms: 6677.407\n",
      "    update_time_ms: 2.611\n",
      "  timestamp: 1665666974\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:16:14,390\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 56056.0x the scale of `vf_clip_param`. This means that it will take more than 56056.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:17 (running for 00:02:09.95)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">          120.03</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -560559</td><td style=\"text-align: right;\">              300079</td><td style=\"text-align: right;\">        -1.79624e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-16-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 446694.00154273777\n",
      "  episode_reward_mean: -511077.95114459743\n",
      "  episode_reward_min: -1796236.4037087257\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 844\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2095117568969727\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01664673537015915\n",
      "          model: {}\n",
      "          policy_loss: -0.005160840228199959\n",
      "          total_loss: 51412041728.0\n",
      "          vf_explained_var: -2.6725953006234704e-08\n",
      "          vf_loss: 51412041728.0\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.344444444444445\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18312598753153972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09052068229904844\n",
      "    mean_inference_ms: 1.0980464699058998\n",
      "    mean_raw_obs_processing_ms: 0.13514616500632537\n",
      "  time_since_restore: 126.70315480232239\n",
      "  time_this_iter_s: 6.672940969467163\n",
      "  time_total_s: 126.70315480232239\n",
      "  timers:\n",
      "    learn_throughput: 1164.606\n",
      "    learn_time_ms: 3434.638\n",
      "    load_throughput: 13161697.654\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 598.339\n",
      "    sample_time_ms: 6685.175\n",
      "    update_time_ms: 2.586\n",
      "  timestamp: 1665666981\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:16:21,110\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 51108.0x the scale of `vf_clip_param`. This means that it will take more than 51108.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:23 (running for 00:02:15.63)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         126.703</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -511078</td><td style=\"text-align: right;\">              446694</td><td style=\"text-align: right;\">        -1.79624e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-16-27\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 446694.00154273777\n",
      "  episode_reward_mean: -525108.9732178384\n",
      "  episode_reward_min: -1655104.5042627323\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 888\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2610549926757812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015034890733659267\n",
      "          model: {}\n",
      "          policy_loss: -0.0064577022567391396\n",
      "          total_loss: 80400367616.0\n",
      "          vf_explained_var: -3.332732756433643e-09\n",
      "          vf_loss: 80400367616.0\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.709999999999999\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18315270810551418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09052479752297402\n",
      "    mean_inference_ms: 1.0983709910971968\n",
      "    mean_raw_obs_processing_ms: 0.1351400145596398\n",
      "  time_since_restore: 133.33050084114075\n",
      "  time_this_iter_s: 6.627346038818359\n",
      "  time_total_s: 133.33050084114075\n",
      "  timers:\n",
      "    learn_throughput: 1163.622\n",
      "    learn_time_ms: 3437.543\n",
      "    load_throughput: 13275214.433\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 599.187\n",
      "    sample_time_ms: 6675.713\n",
      "    update_time_ms: 2.565\n",
      "  timestamp: 1665666987\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:16:27,780\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 52511.0x the scale of `vf_clip_param`. This means that it will take more than 52511.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:28 (running for 00:02:21.31)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         133.331</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -525109</td><td style=\"text-align: right;\">              446694</td><td style=\"text-align: right;\">         -1.6551e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:33 (running for 00:02:26.31)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         133.331</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -525109</td><td style=\"text-align: right;\">              446694</td><td style=\"text-align: right;\">         -1.6551e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-16-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 467802.3196714013\n",
      "  episode_reward_mean: -491050.65240206785\n",
      "  episode_reward_min: -1788541.9934427454\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 932\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1664206981658936\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019246038049459457\n",
      "          model: {}\n",
      "          policy_loss: -0.00854575727134943\n",
      "          total_loss: 70133579776.0\n",
      "          vf_explained_var: 1.2081156341992028e-07\n",
      "          vf_loss: 70133579776.0\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.722222222222221\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1829232988395615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0904143920449247\n",
      "    mean_inference_ms: 1.0973938505198748\n",
      "    mean_raw_obs_processing_ms: 0.13494799044763467\n",
      "  time_since_restore: 139.85438990592957\n",
      "  time_this_iter_s: 6.523889064788818\n",
      "  time_total_s: 139.85438990592957\n",
      "  timers:\n",
      "    learn_throughput: 1164.341\n",
      "    learn_time_ms: 3435.419\n",
      "    load_throughput: 12856104.215\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 599.184\n",
      "    sample_time_ms: 6675.748\n",
      "    update_time_ms: 2.602\n",
      "  timestamp: 1665666994\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:16:34,354\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 49105.0x the scale of `vf_clip_param`. This means that it will take more than 49105.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:39 (running for 00:02:31.93)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         139.854</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -491051</td><td style=\"text-align: right;\">              467802</td><td style=\"text-align: right;\">        -1.78854e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-16-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 467802.3196714013\n",
      "  episode_reward_mean: -534878.2457910257\n",
      "  episode_reward_min: -1842684.6800004775\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 976\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9233801960945129\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01564963534474373\n",
      "          model: {}\n",
      "          policy_loss: -0.0014740020269528031\n",
      "          total_loss: 69072527360.0\n",
      "          vf_explained_var: 7.665285295388458e-08\n",
      "          vf_loss: 69072527360.0\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.209999999999999\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18271660079041901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09031030197337207\n",
      "    mean_inference_ms: 1.0964284451840973\n",
      "    mean_raw_obs_processing_ms: 0.1347590359808445\n",
      "  time_since_restore: 146.53037667274475\n",
      "  time_this_iter_s: 6.6759867668151855\n",
      "  time_total_s: 146.53037667274475\n",
      "  timers:\n",
      "    learn_throughput: 1163.986\n",
      "    learn_time_ms: 3436.467\n",
      "    load_throughput: 13063315.425\n",
      "    load_time_ms: 0.306\n",
      "    sample_throughput: 598.354\n",
      "    sample_time_ms: 6685.011\n",
      "    update_time_ms: 2.561\n",
      "  timestamp: 1665667001\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:16:41,082\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 53488.0x the scale of `vf_clip_param`. This means that it will take more than 53488.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:45 (running for 00:02:37.60)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">          146.53</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -534878</td><td style=\"text-align: right;\">              467802</td><td style=\"text-align: right;\">        -1.84268e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-16-47\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1226365.6760692454\n",
      "  episode_reward_mean: -461744.8513760022\n",
      "  episode_reward_min: -1842684.6800004775\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 1022\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0373921394348145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02319004386663437\n",
      "          model: {}\n",
      "          policy_loss: -0.00530231324955821\n",
      "          total_loss: 83230638080.0\n",
      "          vf_explained_var: 6.666106742159172e-07\n",
      "          vf_loss: 83230638080.0\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.0\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18269739340402855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09031366310575313\n",
      "    mean_inference_ms: 1.0966431906595289\n",
      "    mean_raw_obs_processing_ms: 0.1347546698110865\n",
      "  time_since_restore: 153.25534796714783\n",
      "  time_this_iter_s: 6.724971294403076\n",
      "  time_total_s: 153.25534796714783\n",
      "  timers:\n",
      "    learn_throughput: 1164.682\n",
      "    learn_time_ms: 3434.414\n",
      "    load_throughput: 13301526.996\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 596.47\n",
      "    sample_time_ms: 6706.123\n",
      "    update_time_ms: 2.628\n",
      "  timestamp: 1665667007\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:16:47,847\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 46174.0x the scale of `vf_clip_param`. This means that it will take more than 46174.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:50 (running for 00:02:43.37)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         153.255</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -461745</td><td style=\"text-align: right;\">         1.22637e+06</td><td style=\"text-align: right;\">        -1.84268e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-16-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1226365.6760692454\n",
      "  episode_reward_mean: -540349.7576396645\n",
      "  episode_reward_min: -1676693.1961876422\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1066\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2289685010910034\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011981930583715439\n",
      "          model: {}\n",
      "          policy_loss: -0.005931259598582983\n",
      "          total_loss: 91160756224.0\n",
      "          vf_explained_var: -1.7176391864381912e-08\n",
      "          vf_loss: 91160756224.0\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.377777777777778\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1830095630725804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0904782502738483\n",
      "    mean_inference_ms: 1.0984332482734558\n",
      "    mean_raw_obs_processing_ms: 0.13502346749170313\n",
      "  time_since_restore: 160.11632633209229\n",
      "  time_this_iter_s: 6.860978364944458\n",
      "  time_total_s: 160.11632633209229\n",
      "  timers:\n",
      "    learn_throughput: 1164.724\n",
      "    learn_time_ms: 3434.29\n",
      "    load_throughput: 13304691.515\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 594.565\n",
      "    sample_time_ms: 6727.61\n",
      "    update_time_ms: 2.598\n",
      "  timestamp: 1665667014\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:16:54,751\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 54035.0x the scale of `vf_clip_param`. This means that it will take more than 54035.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:16:56 (running for 00:02:49.32)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         160.116</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\"> -540350</td><td style=\"text-align: right;\">         1.22637e+06</td><td style=\"text-align: right;\">        -1.67669e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-17-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1418308.3046520546\n",
      "  episode_reward_mean: -565245.9410748373\n",
      "  episode_reward_min: -1676693.1961876422\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1110\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7925849556922913\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018444053828716278\n",
      "          model: {}\n",
      "          policy_loss: -0.010590787045657635\n",
      "          total_loss: 93325328384.0\n",
      "          vf_explained_var: 7.721685619799246e-07\n",
      "          vf_loss: 93325328384.0\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.21\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18319251540654996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09056259746122759\n",
      "    mean_inference_ms: 1.0992846003373555\n",
      "    mean_raw_obs_processing_ms: 0.13519320558496478\n",
      "  time_since_restore: 166.70879888534546\n",
      "  time_this_iter_s: 6.592472553253174\n",
      "  time_total_s: 166.70879888534546\n",
      "  timers:\n",
      "    learn_throughput: 1165.091\n",
      "    learn_time_ms: 3433.208\n",
      "    load_throughput: 13322652.267\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 594.306\n",
      "    sample_time_ms: 6730.543\n",
      "    update_time_ms: 2.603\n",
      "  timestamp: 1665667021\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:17:01,395\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 56525.0x the scale of `vf_clip_param`. This means that it will take more than 56525.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:02 (running for 00:02:54.92)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         166.709</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -565246</td><td style=\"text-align: right;\">         1.41831e+06</td><td style=\"text-align: right;\">        -1.67669e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:07 (running for 00:02:59.97)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         166.709</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -565246</td><td style=\"text-align: right;\">         1.41831e+06</td><td style=\"text-align: right;\">        -1.67669e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1418308.3046520546\n",
      "  episode_reward_mean: -544537.8429153509\n",
      "  episode_reward_min: -1500657.906274878\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1154\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8530815243721008\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013803554698824883\n",
      "          model: {}\n",
      "          policy_loss: -0.010995196178555489\n",
      "          total_loss: 73660809216.0\n",
      "          vf_explained_var: 2.436740373923385e-07\n",
      "          vf_loss: 73660809216.0\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.35\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1832506931336436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09059008064992537\n",
      "    mean_inference_ms: 1.0997795548622442\n",
      "    mean_raw_obs_processing_ms: 0.1352629477256853\n",
      "  time_since_restore: 173.5020875930786\n",
      "  time_this_iter_s: 6.793288707733154\n",
      "  time_total_s: 173.5020875930786\n",
      "  timers:\n",
      "    learn_throughput: 1167.027\n",
      "    learn_time_ms: 3427.512\n",
      "    load_throughput: 12753489.928\n",
      "    load_time_ms: 0.314\n",
      "    sample_throughput: 592.826\n",
      "    sample_time_ms: 6747.342\n",
      "    update_time_ms: 2.625\n",
      "  timestamp: 1665667028\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:17:08,235\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 54454.0x the scale of `vf_clip_param`. This means that it will take more than 54454.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:13 (running for 00:03:05.76)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         173.502</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -544538</td><td style=\"text-align: right;\">         1.41831e+06</td><td style=\"text-align: right;\">        -1.50066e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-17-15\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 796858.5177988058\n",
      "  episode_reward_mean: -523142.794917044\n",
      "  episode_reward_min: -1467447.3600760477\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9279830455780029\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015081815421581268\n",
      "          model: {}\n",
      "          policy_loss: -0.007046897429972887\n",
      "          total_loss: 58025521152.0\n",
      "          vf_explained_var: 2.8924276307407126e-07\n",
      "          vf_loss: 58025521152.0\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.47777777777778\n",
      "    ram_util_percent: 17.299999999999997\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18338866019205702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09066824724933836\n",
      "    mean_inference_ms: 1.100858112844633\n",
      "    mean_raw_obs_processing_ms: 0.13539153168737664\n",
      "  time_since_restore: 180.23769688606262\n",
      "  time_this_iter_s: 6.735609292984009\n",
      "  time_total_s: 180.23769688606262\n",
      "  timers:\n",
      "    learn_throughput: 1168.789\n",
      "    learn_time_ms: 3422.347\n",
      "    load_throughput: 12160927.805\n",
      "    load_time_ms: 0.329\n",
      "    sample_throughput: 592.746\n",
      "    sample_time_ms: 6748.256\n",
      "    update_time_ms: 2.59\n",
      "  timestamp: 1665667035\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:17:15,022\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 52314.0x the scale of `vf_clip_param`. This means that it will take more than 52314.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:19 (running for 00:03:11.60)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         180.238</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -523143</td><td style=\"text-align: right;\">              796859</td><td style=\"text-align: right;\">        -1.46745e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-17-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 734215.4955729733\n",
      "  episode_reward_mean: -545115.6196264599\n",
      "  episode_reward_min: -1689973.8045784873\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1244\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9313879609107971\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01690497249364853\n",
      "          model: {}\n",
      "          policy_loss: -0.004634142387658358\n",
      "          total_loss: 73055068160.0\n",
      "          vf_explained_var: 3.387210085747938e-07\n",
      "          vf_loss: 73055068160.0\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.430000000000001\n",
      "    ram_util_percent: 17.15\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18349751261816563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09072061758472665\n",
      "    mean_inference_ms: 1.1013571144946843\n",
      "    mean_raw_obs_processing_ms: 0.13547582918246143\n",
      "  time_since_restore: 186.84378051757812\n",
      "  time_this_iter_s: 6.606083631515503\n",
      "  time_total_s: 186.84378051757812\n",
      "  timers:\n",
      "    learn_throughput: 1168.854\n",
      "    learn_time_ms: 3422.155\n",
      "    load_throughput: 11787547.249\n",
      "    load_time_ms: 0.339\n",
      "    sample_throughput: 594.416\n",
      "    sample_time_ms: 6729.289\n",
      "    update_time_ms: 2.578\n",
      "  timestamp: 1665667041\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:17:21,681\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 54512.0x the scale of `vf_clip_param`. This means that it will take more than 54512.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:24 (running for 00:03:17.20)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         186.844</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -545116</td><td style=\"text-align: right;\">              734215</td><td style=\"text-align: right;\">        -1.68997e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-17-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 734215.4955729733\n",
      "  episode_reward_mean: -567289.0242905404\n",
      "  episode_reward_min: -1689973.8045784873\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1288\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9556253552436829\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016288822516798973\n",
      "          model: {}\n",
      "          policy_loss: -0.008649468421936035\n",
      "          total_loss: 72932597760.0\n",
      "          vf_explained_var: 4.338320991337241e-07\n",
      "          vf_loss: 72932597760.0\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.28888888888889\n",
      "    ram_util_percent: 17.37777777777778\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1834201578899534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09066802873704867\n",
      "    mean_inference_ms: 1.1005544125288782\n",
      "    mean_raw_obs_processing_ms: 0.13541221197349265\n",
      "  time_since_restore: 193.34708261489868\n",
      "  time_this_iter_s: 6.503302097320557\n",
      "  time_total_s: 193.34708261489868\n",
      "  timers:\n",
      "    learn_throughput: 1168.996\n",
      "    learn_time_ms: 3421.74\n",
      "    load_throughput: 11780098.301\n",
      "    load_time_ms: 0.34\n",
      "    sample_throughput: 595.967\n",
      "    sample_time_ms: 6711.786\n",
      "    update_time_ms: 2.537\n",
      "  timestamp: 1665667048\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:17:28,223\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 56729.0x the scale of `vf_clip_param`. This means that it will take more than 56729.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:30 (running for 00:03:22.80)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         193.347</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -567289</td><td style=\"text-align: right;\">              734215</td><td style=\"text-align: right;\">        -1.68997e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-17-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1050550.285849809\n",
      "  episode_reward_mean: -542694.1779796224\n",
      "  episode_reward_min: -1540854.3877303253\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1332\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9944259524345398\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0139922508969903\n",
      "          model: {}\n",
      "          policy_loss: -0.003104197094216943\n",
      "          total_loss: 87390265344.0\n",
      "          vf_explained_var: 4.733762466457847e-07\n",
      "          vf_loss: 87390265344.0\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.22\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18322933733949323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09057164060154202\n",
      "    mean_inference_ms: 1.0995394176755304\n",
      "    mean_raw_obs_processing_ms: 0.1352697501211727\n",
      "  time_since_restore: 199.92514753341675\n",
      "  time_this_iter_s: 6.578064918518066\n",
      "  time_total_s: 199.92514753341675\n",
      "  timers:\n",
      "    learn_throughput: 1170.515\n",
      "    learn_time_ms: 3417.3\n",
      "    load_throughput: 12297307.044\n",
      "    load_time_ms: 0.325\n",
      "    sample_throughput: 596.064\n",
      "    sample_time_ms: 6710.685\n",
      "    update_time_ms: 2.533\n",
      "  timestamp: 1665667054\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:17:34,842\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 54269.0x the scale of `vf_clip_param`. This means that it will take more than 54269.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:35 (running for 00:03:28.37)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         199.925</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -542694</td><td style=\"text-align: right;\">         1.05055e+06</td><td style=\"text-align: right;\">        -1.54085e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:40 (running for 00:03:33.41)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         199.925</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -542694</td><td style=\"text-align: right;\">         1.05055e+06</td><td style=\"text-align: right;\">        -1.54085e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-17-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1050550.285849809\n",
      "  episode_reward_mean: -543105.0704736608\n",
      "  episode_reward_min: -1540854.3877303253\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1376\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7603514194488525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01589028909802437\n",
      "          model: {}\n",
      "          policy_loss: -0.010823415592312813\n",
      "          total_loss: 61033439232.0\n",
      "          vf_explained_var: -1.4424964547288255e-06\n",
      "          vf_loss: 61033439232.0\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.322222222222223\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18308864738633102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0904936786785198\n",
      "    mean_inference_ms: 1.0990383456485024\n",
      "    mean_raw_obs_processing_ms: 0.13515684212365955\n",
      "  time_since_restore: 206.51551723480225\n",
      "  time_this_iter_s: 6.590369701385498\n",
      "  time_total_s: 206.51551723480225\n",
      "  timers:\n",
      "    learn_throughput: 1171.181\n",
      "    learn_time_ms: 3415.355\n",
      "    load_throughput: 12438623.962\n",
      "    load_time_ms: 0.322\n",
      "    sample_throughput: 595.678\n",
      "    sample_time_ms: 6715.033\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1665667061\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:17:41,483\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 54311.0x the scale of `vf_clip_param`. This means that it will take more than 54311.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:46 (running for 00:03:39.01)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         206.516</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -543105</td><td style=\"text-align: right;\">         1.05055e+06</td><td style=\"text-align: right;\">        -1.54085e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-17-48\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 417773.5048729749\n",
      "  episode_reward_mean: -597563.2949581489\n",
      "  episode_reward_min: -1729356.8193638474\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 1422\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9844431281089783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019719691947102547\n",
      "          model: {}\n",
      "          policy_loss: -0.007763024419546127\n",
      "          total_loss: 89834635264.0\n",
      "          vf_explained_var: 8.16519545310257e-08\n",
      "          vf_loss: 89834635264.0\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.44\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18302948754174736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0904730091428381\n",
      "    mean_inference_ms: 1.0991807754845784\n",
      "    mean_raw_obs_processing_ms: 0.13510713244827852\n",
      "  time_since_restore: 213.27009415626526\n",
      "  time_this_iter_s: 6.754576921463013\n",
      "  time_total_s: 213.27009415626526\n",
      "  timers:\n",
      "    learn_throughput: 1171.898\n",
      "    learn_time_ms: 3413.267\n",
      "    load_throughput: 12664917.34\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 595.032\n",
      "    sample_time_ms: 6722.331\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1665667068\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:17:48,284\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 59756.0x the scale of `vf_clip_param`. This means that it will take more than 59756.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:52 (running for 00:03:44.84)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">          213.27</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -597563</td><td style=\"text-align: right;\">              417774</td><td style=\"text-align: right;\">        -1.72936e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-17-55\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 417773.5048729749\n",
      "  episode_reward_mean: -623869.6735708886\n",
      "  episode_reward_min: -1729356.8193638474\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1466\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.167571783065796\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017205605283379555\n",
      "          model: {}\n",
      "          policy_loss: -0.013142027892172337\n",
      "          total_loss: 79281127424.0\n",
      "          vf_explained_var: 2.548899828980211e-07\n",
      "          vf_loss: 79281127424.0\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.344444444444443\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1830367207336218\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09048342662735717\n",
      "    mean_inference_ms: 1.0994079605017155\n",
      "    mean_raw_obs_processing_ms: 0.1351277806265605\n",
      "  time_since_restore: 220.00643229484558\n",
      "  time_this_iter_s: 6.736338138580322\n",
      "  time_total_s: 220.00643229484558\n",
      "  timers:\n",
      "    learn_throughput: 1168.433\n",
      "    learn_time_ms: 3423.389\n",
      "    load_throughput: 12838396.082\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 596.012\n",
      "    sample_time_ms: 6711.277\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1665667075\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:17:55,060\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 62387.0x the scale of `vf_clip_param`. This means that it will take more than 62387.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:17:58 (running for 00:03:50.59)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         220.006</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -623870</td><td style=\"text-align: right;\">              417774</td><td style=\"text-align: right;\">        -1.72936e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-18-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 590832.8549305144\n",
      "  episode_reward_mean: -593020.6814112059\n",
      "  episode_reward_min: -1631772.7760187835\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1510\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8403895497322083\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024707404896616936\n",
      "          model: {}\n",
      "          policy_loss: -0.006673905067145824\n",
      "          total_loss: 72888811520.0\n",
      "          vf_explained_var: 2.530697884139954e-06\n",
      "          vf_loss: 72888811520.0\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.059999999999999\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18298049784979084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09045161843597413\n",
      "    mean_inference_ms: 1.0990913320966036\n",
      "    mean_raw_obs_processing_ms: 0.13509219138865267\n",
      "  time_since_restore: 226.58395624160767\n",
      "  time_this_iter_s: 6.577523946762085\n",
      "  time_total_s: 226.58395624160767\n",
      "  timers:\n",
      "    learn_throughput: 1168.298\n",
      "    learn_time_ms: 3423.784\n",
      "    load_throughput: 12890676.911\n",
      "    load_time_ms: 0.31\n",
      "    sample_throughput: 597.619\n",
      "    sample_time_ms: 6693.222\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1665667081\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:18:01,687\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 59302.0x the scale of `vf_clip_param`. This means that it will take more than 59302.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:03 (running for 00:03:56.20)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         226.584</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -593021</td><td style=\"text-align: right;\">              590833</td><td style=\"text-align: right;\">        -1.63177e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:18:08,428\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 47439.0x the scale of `vf_clip_param`. This means that it will take more than 47439.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-18-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1028207.0261846854\n",
      "  episode_reward_mean: -474385.49343460525\n",
      "  episode_reward_min: -1611213.5873367097\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1554\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.792876124382019\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013953977264463902\n",
      "          model: {}\n",
      "          policy_loss: -0.010120890103280544\n",
      "          total_loss: 68254724096.0\n",
      "          vf_explained_var: 2.425075763312634e-06\n",
      "          vf_loss: 68254724096.0\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.233333333333333\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1830129120367795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09047480138216159\n",
      "    mean_inference_ms: 1.0994692200091283\n",
      "    mean_raw_obs_processing_ms: 0.13511526679620822\n",
      "  time_since_restore: 233.28824710845947\n",
      "  time_this_iter_s: 6.704290866851807\n",
      "  time_total_s: 233.28824710845947\n",
      "  timers:\n",
      "    learn_throughput: 1168.309\n",
      "    learn_time_ms: 3423.751\n",
      "    load_throughput: 12875837.299\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 596.706\n",
      "    sample_time_ms: 6703.471\n",
      "    update_time_ms: 2.341\n",
      "  timestamp: 1665667088\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:09 (running for 00:04:01.96)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         233.288</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -474385</td><td style=\"text-align: right;\">         1.02821e+06</td><td style=\"text-align: right;\">        -1.61121e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:14 (running for 00:04:07.00)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         233.288</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -474385</td><td style=\"text-align: right;\">         1.02821e+06</td><td style=\"text-align: right;\">        -1.61121e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:18:15,104\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 50598.0x the scale of `vf_clip_param`. This means that it will take more than 50598.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-18-15\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1028207.0261846854\n",
      "  episode_reward_mean: -505980.5721527807\n",
      "  episode_reward_min: -1490066.3214502528\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 1600\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3889199495315552\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01107168197631836\n",
      "          model: {}\n",
      "          policy_loss: -0.007677171379327774\n",
      "          total_loss: 73780125696.0\n",
      "          vf_explained_var: 5.65218670089962e-07\n",
      "          vf_loss: 73780125696.0\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.22\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18309370813580153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09053619636661821\n",
      "    mean_inference_ms: 1.1002532317382245\n",
      "    mean_raw_obs_processing_ms: 0.13515913554566483\n",
      "  time_since_restore: 239.91158747673035\n",
      "  time_this_iter_s: 6.623340368270874\n",
      "  time_total_s: 239.91158747673035\n",
      "  timers:\n",
      "    learn_throughput: 1168.831\n",
      "    learn_time_ms: 3422.223\n",
      "    load_throughput: 13535470.754\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 598.04\n",
      "    sample_time_ms: 6688.513\n",
      "    update_time_ms: 2.28\n",
      "  timestamp: 1665667095\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:20 (running for 00:04:12.63)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         239.912</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -505981</td><td style=\"text-align: right;\">         1.02821e+06</td><td style=\"text-align: right;\">        -1.49007e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-18-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 576404.2576262727\n",
      "  episode_reward_mean: -524512.8649079911\n",
      "  episode_reward_min: -1490066.3214502528\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1644\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2192542552947998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013070781715214252\n",
      "          model: {}\n",
      "          policy_loss: -0.009513742290437222\n",
      "          total_loss: 59804704768.0\n",
      "          vf_explained_var: 1.3455909311232972e-06\n",
      "          vf_loss: 59804704768.0\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.355555555555556\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18307172023415907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09053384845087853\n",
      "    mean_inference_ms: 1.10028262950409\n",
      "    mean_raw_obs_processing_ms: 0.1351127324302368\n",
      "  time_since_restore: 246.5321228504181\n",
      "  time_this_iter_s: 6.620535373687744\n",
      "  time_total_s: 246.5321228504181\n",
      "  timers:\n",
      "    learn_throughput: 1168.871\n",
      "    learn_time_ms: 3422.105\n",
      "    load_throughput: 14144857.938\n",
      "    load_time_ms: 0.283\n",
      "    sample_throughput: 599.198\n",
      "    sample_time_ms: 6675.594\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1665667101\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:18:21,776\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 52451.0x the scale of `vf_clip_param`. This means that it will take more than 52451.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:25 (running for 00:04:18.30)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         246.532</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -524513</td><td style=\"text-align: right;\">              576404</td><td style=\"text-align: right;\">        -1.49007e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 610916.7172744976\n",
      "  episode_reward_mean: -540761.7518901916\n",
      "  episode_reward_min: -1657679.5771603037\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1688\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.971340537071228\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01464129239320755\n",
      "          model: {}\n",
      "          policy_loss: -0.003106914460659027\n",
      "          total_loss: 79388295168.0\n",
      "          vf_explained_var: 1.5169702010098263e-06\n",
      "          vf_loss: 79388295168.0\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.27\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18309609145515707\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09053983930351783\n",
      "    mean_inference_ms: 1.100389768056648\n",
      "    mean_raw_obs_processing_ms: 0.1351011208613032\n",
      "  time_since_restore: 253.2830536365509\n",
      "  time_this_iter_s: 6.7509307861328125\n",
      "  time_total_s: 253.2830536365509\n",
      "  timers:\n",
      "    learn_throughput: 1168.896\n",
      "    learn_time_ms: 3422.034\n",
      "    load_throughput: 14169945.946\n",
      "    load_time_ms: 0.282\n",
      "    sample_throughput: 598.006\n",
      "    sample_time_ms: 6688.897\n",
      "    update_time_ms: 2.295\n",
      "  timestamp: 1665667108\n",
      "  timesteps_since_restore: 152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:18:28,569\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 54076.0x the scale of `vf_clip_param`. This means that it will take more than 54076.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:31 (running for 00:04:24.13)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         253.283</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -540762</td><td style=\"text-align: right;\">              610917</td><td style=\"text-align: right;\">        -1.65768e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-18-35\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 686490.8777731272\n",
      "  episode_reward_mean: -533176.056646848\n",
      "  episode_reward_min: -1924091.5839179684\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1732\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4315719604492188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020326396450400352\n",
      "          model: {}\n",
      "          policy_loss: -0.013157771900296211\n",
      "          total_loss: 88121860096.0\n",
      "          vf_explained_var: 4.521172741078772e-06\n",
      "          vf_loss: 88121860096.0\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.488888888888889\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18312100077453997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09055609767662216\n",
      "    mean_inference_ms: 1.1006305478326484\n",
      "    mean_raw_obs_processing_ms: 0.13509077170350683\n",
      "  time_since_restore: 259.80523562431335\n",
      "  time_this_iter_s: 6.522181987762451\n",
      "  time_total_s: 259.80523562431335\n",
      "  timers:\n",
      "    learn_throughput: 1168.881\n",
      "    learn_time_ms: 3422.075\n",
      "    load_throughput: 14493102.972\n",
      "    load_time_ms: 0.276\n",
      "    sample_throughput: 597.78\n",
      "    sample_time_ms: 6691.426\n",
      "    update_time_ms: 2.368\n",
      "  timestamp: 1665667115\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:18:35,139\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 53318.0x the scale of `vf_clip_param`. This means that it will take more than 53318.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:37 (running for 00:04:29.67)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         259.805</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -533176</td><td style=\"text-align: right;\">              686491</td><td style=\"text-align: right;\">        -1.92409e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-18-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 840074.3753991356\n",
      "  episode_reward_mean: -553065.464105945\n",
      "  episode_reward_min: -1924091.5839179684\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1776\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3192152976989746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014033311046659946\n",
      "          model: {}\n",
      "          policy_loss: -0.005511116702109575\n",
      "          total_loss: 81956790272.0\n",
      "          vf_explained_var: 1.3907109632782522e-06\n",
      "          vf_loss: 81956790272.0\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.24\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1830763118003706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09053040484287866\n",
      "    mean_inference_ms: 1.1003980692234636\n",
      "    mean_raw_obs_processing_ms: 0.13503447126158108\n",
      "  time_since_restore: 266.462815284729\n",
      "  time_this_iter_s: 6.657579660415649\n",
      "  time_total_s: 266.462815284729\n",
      "  timers:\n",
      "    learn_throughput: 1168.61\n",
      "    learn_time_ms: 3422.87\n",
      "    load_throughput: 13676706.611\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 597.047\n",
      "    sample_time_ms: 6699.641\n",
      "    update_time_ms: 2.363\n",
      "  timestamp: 1665667121\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:18:41,848\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 55307.0x the scale of `vf_clip_param`. This means that it will take more than 55307.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:42 (running for 00:04:35.43)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         266.463</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -553065</td><td style=\"text-align: right;\">              840074</td><td style=\"text-align: right;\">        -1.92409e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:47 (running for 00:04:40.44)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         266.463</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -553065</td><td style=\"text-align: right;\">              840074</td><td style=\"text-align: right;\">        -1.92409e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-18-48\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 840074.3753991356\n",
      "  episode_reward_mean: -580228.1608491829\n",
      "  episode_reward_min: -1477675.6102951977\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 1822\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0773252248764038\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016354158520698547\n",
      "          model: {}\n",
      "          policy_loss: -0.008646323345601559\n",
      "          total_loss: 79362351104.0\n",
      "          vf_explained_var: 3.6303074466559337e-06\n",
      "          vf_loss: 79362351104.0\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.21111111111111\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1828848144243502\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09044083523053197\n",
      "    mean_inference_ms: 1.099476185105907\n",
      "    mean_raw_obs_processing_ms: 0.13488397969560512\n",
      "  time_since_restore: 272.953857421875\n",
      "  time_this_iter_s: 6.491042137145996\n",
      "  time_total_s: 272.953857421875\n",
      "  timers:\n",
      "    learn_throughput: 1169.201\n",
      "    learn_time_ms: 3421.139\n",
      "    load_throughput: 14319918.061\n",
      "    load_time_ms: 0.279\n",
      "    sample_throughput: 597.671\n",
      "    sample_time_ms: 6692.65\n",
      "    update_time_ms: 2.366\n",
      "  timestamp: 1665667128\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:18:48,393\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 58023.0x the scale of `vf_clip_param`. This means that it will take more than 58023.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:53 (running for 00:04:45.95)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         272.954</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -580228</td><td style=\"text-align: right;\">              840074</td><td style=\"text-align: right;\">        -1.47768e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-18-55\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 840074.3753991356\n",
      "  episode_reward_mean: -566495.1301646404\n",
      "  episode_reward_min: -1413060.5377259837\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1866\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9683565497398376\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014610319398343563\n",
      "          model: {}\n",
      "          policy_loss: -0.019647376611828804\n",
      "          total_loss: 54797955072.0\n",
      "          vf_explained_var: 5.4376100706576835e-06\n",
      "          vf_loss: 54797955072.0\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.21\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1827883453520456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09039083789186339\n",
      "    mean_inference_ms: 1.0989183935723859\n",
      "    mean_raw_obs_processing_ms: 0.13480337124579747\n",
      "  time_since_restore: 279.5559730529785\n",
      "  time_this_iter_s: 6.602115631103516\n",
      "  time_total_s: 279.5559730529785\n",
      "  timers:\n",
      "    learn_throughput: 1169.274\n",
      "    learn_time_ms: 3420.927\n",
      "    load_throughput: 14318695.912\n",
      "    load_time_ms: 0.279\n",
      "    sample_throughput: 599.176\n",
      "    sample_time_ms: 6675.83\n",
      "    update_time_ms: 2.388\n",
      "  timestamp: 1665667135\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:18:55,040\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 56650.0x the scale of `vf_clip_param`. This means that it will take more than 56650.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:18:59 (running for 00:04:51.57)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         279.556</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -566495</td><td style=\"text-align: right;\">              840074</td><td style=\"text-align: right;\">        -1.41306e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-19-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 344701.0840516987\n",
      "  episode_reward_mean: -533837.2797913863\n",
      "  episode_reward_min: -1491739.3491349455\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1910\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2763068675994873\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012524193152785301\n",
      "          model: {}\n",
      "          policy_loss: -0.009090246632695198\n",
      "          total_loss: 77856874496.0\n",
      "          vf_explained_var: 3.117835603916319e-06\n",
      "          vf_loss: 77856874496.0\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.61111111111111\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1827322388333889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09035318526783347\n",
      "    mean_inference_ms: 1.098386747175437\n",
      "    mean_raw_obs_processing_ms: 0.13474481118110881\n",
      "  time_since_restore: 285.9972641468048\n",
      "  time_this_iter_s: 6.441291093826294\n",
      "  time_total_s: 285.9972641468048\n",
      "  timers:\n",
      "    learn_throughput: 1173.497\n",
      "    learn_time_ms: 3408.615\n",
      "    load_throughput: 14702669.354\n",
      "    load_time_ms: 0.272\n",
      "    sample_throughput: 600.646\n",
      "    sample_time_ms: 6659.491\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1665667141\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:19:01,531\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 53384.0x the scale of `vf_clip_param`. This means that it will take more than 53384.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:04 (running for 00:04:57.09)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         285.997</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> -533837</td><td style=\"text-align: right;\">              344701</td><td style=\"text-align: right;\">        -1.49174e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-19-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 344701.0840516987\n",
      "  episode_reward_mean: -534007.6672725537\n",
      "  episode_reward_min: -1667528.0376012407\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 1954\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2731634378433228\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01067464891821146\n",
      "          model: {}\n",
      "          policy_loss: -0.00925455242395401\n",
      "          total_loss: 61618692096.0\n",
      "          vf_explained_var: 2.486410949131823e-06\n",
      "          vf_loss: 61618692096.0\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.52\n",
      "    ram_util_percent: 17.4\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18270028843324113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09034052636274956\n",
      "    mean_inference_ms: 1.0982665777379155\n",
      "    mean_raw_obs_processing_ms: 0.1347127848333746\n",
      "  time_since_restore: 292.6977834701538\n",
      "  time_this_iter_s: 6.700519323348999\n",
      "  time_total_s: 292.6977834701538\n",
      "  timers:\n",
      "    learn_throughput: 1175.037\n",
      "    learn_time_ms: 3404.149\n",
      "    load_throughput: 15220190.511\n",
      "    load_time_ms: 0.263\n",
      "    sample_throughput: 600.264\n",
      "    sample_time_ms: 6663.735\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1665667148\n",
      "  timesteps_since_restore: 176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:19:08,280\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 53401.0x the scale of `vf_clip_param`. This means that it will take more than 53401.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:10 (running for 00:05:02.80)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         292.698</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -534008</td><td style=\"text-align: right;\">              344701</td><td style=\"text-align: right;\">        -1.66753e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-19-15\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 665037.4452635654\n",
      "  episode_reward_mean: -515238.9234274429\n",
      "  episode_reward_min: -1667528.0376012407\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 2000\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2655081748962402\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013752976432442665\n",
      "          model: {}\n",
      "          policy_loss: -0.0056557063944637775\n",
      "          total_loss: 71140556800.0\n",
      "          vf_explained_var: 1.7282142152907909e-06\n",
      "          vf_loss: 71140556800.0\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.355555555555556\n",
      "    ram_util_percent: 17.41111111111111\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18272578725311334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09036604403441671\n",
      "    mean_inference_ms: 1.0986986656029847\n",
      "    mean_raw_obs_processing_ms: 0.1347196266526241\n",
      "  time_since_restore: 299.46615982055664\n",
      "  time_this_iter_s: 6.768376350402832\n",
      "  time_total_s: 299.46615982055664\n",
      "  timers:\n",
      "    learn_throughput: 1175.435\n",
      "    learn_time_ms: 3402.995\n",
      "    load_throughput: 15246470.374\n",
      "    load_time_ms: 0.262\n",
      "    sample_throughput: 599.879\n",
      "    sample_time_ms: 6668.012\n",
      "    update_time_ms: 2.384\n",
      "  timestamp: 1665667155\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:19:15,097\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 51524.0x the scale of `vf_clip_param`. This means that it will take more than 51524.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:16 (running for 00:05:08.62)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         299.466</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -515239</td><td style=\"text-align: right;\">              665037</td><td style=\"text-align: right;\">        -1.66753e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:21 (running for 00:05:13.63)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         299.466</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -515239</td><td style=\"text-align: right;\">              665037</td><td style=\"text-align: right;\">        -1.66753e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-19-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 665037.4452635654\n",
      "  episode_reward_mean: -558330.478673671\n",
      "  episode_reward_min: -1681611.6888419595\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2044\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1682051420211792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014523990452289581\n",
      "          model: {}\n",
      "          policy_loss: -0.007954519242048264\n",
      "          total_loss: 68958658560.0\n",
      "          vf_explained_var: 4.237697908138216e-07\n",
      "          vf_loss: 68958658560.0\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.440000000000001\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18273124723069833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09036715207087408\n",
      "    mean_inference_ms: 1.0988144813729612\n",
      "    mean_raw_obs_processing_ms: 0.13471298255786554\n",
      "  time_since_restore: 306.08538007736206\n",
      "  time_this_iter_s: 6.61922025680542\n",
      "  time_total_s: 306.08538007736206\n",
      "  timers:\n",
      "    learn_throughput: 1175.536\n",
      "    learn_time_ms: 3402.702\n",
      "    load_throughput: 14439466.391\n",
      "    load_time_ms: 0.277\n",
      "    sample_throughput: 600.022\n",
      "    sample_time_ms: 6666.427\n",
      "    update_time_ms: 2.371\n",
      "  timestamp: 1665667161\n",
      "  timesteps_since_restore: 184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:19:21,765\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 55833.0x the scale of `vf_clip_param`. This means that it will take more than 55833.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:26 (running for 00:05:19.33)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         306.085</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -558330</td><td style=\"text-align: right;\">              665037</td><td style=\"text-align: right;\">        -1.68161e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-19-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 290641.8287574219\n",
      "  episode_reward_mean: -617451.6142805059\n",
      "  episode_reward_min: -1755979.0436835508\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2088\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.043968677520752\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017597584053874016\n",
      "          model: {}\n",
      "          policy_loss: -0.005321233067661524\n",
      "          total_loss: 81426644992.0\n",
      "          vf_explained_var: 1.6085306924651377e-05\n",
      "          vf_loss: 81426644992.0\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.411111111111111\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18267124671323756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09032467668248571\n",
      "    mean_inference_ms: 1.0983969949926242\n",
      "    mean_raw_obs_processing_ms: 0.1346537565474279\n",
      "  time_since_restore: 312.6141903400421\n",
      "  time_this_iter_s: 6.528810262680054\n",
      "  time_total_s: 312.6141903400421\n",
      "  timers:\n",
      "    learn_throughput: 1175.531\n",
      "    learn_time_ms: 3402.719\n",
      "    load_throughput: 13851730.515\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 600.933\n",
      "    sample_time_ms: 6656.313\n",
      "    update_time_ms: 2.407\n",
      "  timestamp: 1665667168\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:19:28,339\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 61745.0x the scale of `vf_clip_param`. This means that it will take more than 61745.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:32 (running for 00:05:24.87)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         312.614</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -617452</td><td style=\"text-align: right;\">              290642</td><td style=\"text-align: right;\">        -1.75598e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-19-35\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 649527.8106908791\n",
      "  episode_reward_mean: -602715.4797247009\n",
      "  episode_reward_min: -1755979.0436835508\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2132\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1911612749099731\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01987648755311966\n",
      "          model: {}\n",
      "          policy_loss: -0.006016653496772051\n",
      "          total_loss: 95620063232.0\n",
      "          vf_explained_var: 4.0915065255830996e-06\n",
      "          vf_loss: 95620063232.0\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.260000000000002\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18253038238524946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09025127848330872\n",
      "    mean_inference_ms: 1.0977414153122504\n",
      "    mean_raw_obs_processing_ms: 0.13454386335892904\n",
      "  time_since_restore: 319.25812458992004\n",
      "  time_this_iter_s: 6.64393424987793\n",
      "  time_total_s: 319.25812458992004\n",
      "  timers:\n",
      "    learn_throughput: 1175.171\n",
      "    learn_time_ms: 3403.76\n",
      "    load_throughput: 13115397.123\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 601.933\n",
      "    sample_time_ms: 6645.261\n",
      "    update_time_ms: 2.438\n",
      "  timestamp: 1665667175\n",
      "  timesteps_since_restore: 192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:19:35,033\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 60272.0x the scale of `vf_clip_param`. This means that it will take more than 60272.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:38 (running for 00:05:30.56)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         319.258</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> -602715</td><td style=\"text-align: right;\">              649528</td><td style=\"text-align: right;\">        -1.75598e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-19-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 649527.8106908791\n",
      "  episode_reward_mean: -569948.20671121\n",
      "  episode_reward_min: -1683402.220587551\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2176\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0197174549102783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013437775894999504\n",
      "          model: {}\n",
      "          policy_loss: -0.002057990524917841\n",
      "          total_loss: 74447978496.0\n",
      "          vf_explained_var: -2.297983428434236e-06\n",
      "          vf_loss: 74447978496.0\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.466666666666667\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1824856598198142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09022836682196153\n",
      "    mean_inference_ms: 1.0975430836479296\n",
      "    mean_raw_obs_processing_ms: 0.13451538341672492\n",
      "  time_since_restore: 325.8777494430542\n",
      "  time_this_iter_s: 6.619624853134155\n",
      "  time_total_s: 325.8777494430542\n",
      "  timers:\n",
      "    learn_throughput: 1175.146\n",
      "    learn_time_ms: 3403.832\n",
      "    load_throughput: 12852164.854\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 600.958\n",
      "    sample_time_ms: 6656.04\n",
      "    update_time_ms: 2.421\n",
      "  timestamp: 1665667181\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:19:41,700\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 56995.0x the scale of `vf_clip_param`. This means that it will take more than 56995.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:43 (running for 00:05:36.23)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         325.878</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> -569948</td><td style=\"text-align: right;\">              649528</td><td style=\"text-align: right;\">         -1.6834e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-19-48\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 928936.5279323326\n",
      "  episode_reward_mean: -435436.87515079154\n",
      "  episode_reward_min: -1683402.220587551\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 2222\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7162348031997681\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016495879739522934\n",
      "          model: {}\n",
      "          policy_loss: -0.003664983669295907\n",
      "          total_loss: 56388587520.0\n",
      "          vf_explained_var: -2.0509125064904765e-08\n",
      "          vf_loss: 56388587520.0\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.25\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18232570782053487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0901597151152714\n",
      "    mean_inference_ms: 1.0969228139549698\n",
      "    mean_raw_obs_processing_ms: 0.1343980776412007\n",
      "  time_since_restore: 332.3850305080414\n",
      "  time_this_iter_s: 6.507281064987183\n",
      "  time_total_s: 332.3850305080414\n",
      "  timers:\n",
      "    learn_throughput: 1175.134\n",
      "    learn_time_ms: 3403.866\n",
      "    load_throughput: 12854134.232\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 602.305\n",
      "    sample_time_ms: 6641.151\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1665667188\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:19:48,259\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 43544.0x the scale of `vf_clip_param`. This means that it will take more than 43544.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:49 (running for 00:05:41.78)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         332.385</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> -435437</td><td style=\"text-align: right;\">              928937</td><td style=\"text-align: right;\">         -1.6834e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:19:54 (running for 00:05:46.78)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         332.385</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> -435437</td><td style=\"text-align: right;\">              928937</td><td style=\"text-align: right;\">         -1.6834e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-19-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1336841.7913551782\n",
      "  episode_reward_mean: -408877.78242654674\n",
      "  episode_reward_min: -1683402.220587551\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2266\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.172021746635437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015045561827719212\n",
      "          model: {}\n",
      "          policy_loss: -0.00521119823679328\n",
      "          total_loss: 68561772544.0\n",
      "          vf_explained_var: 1.264387606170203e-06\n",
      "          vf_loss: 68561772544.0\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.122222222222222\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18218882162720704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09009163769220387\n",
      "    mean_inference_ms: 1.0963061423374025\n",
      "    mean_raw_obs_processing_ms: 0.13429020726963659\n",
      "  time_since_restore: 338.9941840171814\n",
      "  time_this_iter_s: 6.609153509140015\n",
      "  time_total_s: 338.9941840171814\n",
      "  timers:\n",
      "    learn_throughput: 1174.853\n",
      "    learn_time_ms: 3404.682\n",
      "    load_throughput: 12744770.586\n",
      "    load_time_ms: 0.314\n",
      "    sample_throughput: 601.445\n",
      "    sample_time_ms: 6650.653\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1665667194\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:19:54,908\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 40888.0x the scale of `vf_clip_param`. This means that it will take more than 40888.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:00 (running for 00:05:52.49)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         338.994</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> -408878</td><td style=\"text-align: right;\">         1.33684e+06</td><td style=\"text-align: right;\">         -1.6834e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-20-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1336841.7913551782\n",
      "  episode_reward_mean: -434701.939660407\n",
      "  episode_reward_min: -1472353.5285271378\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2310\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.007601261138916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013740590773522854\n",
      "          model: {}\n",
      "          policy_loss: -0.009714437648653984\n",
      "          total_loss: 75265990656.0\n",
      "          vf_explained_var: -2.773263076960575e-05\n",
      "          vf_loss: 75265990656.0\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.3\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1821185474477396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09004635741170397\n",
      "    mean_inference_ms: 1.095926541969019\n",
      "    mean_raw_obs_processing_ms: 0.13422977434350897\n",
      "  time_since_restore: 345.63842487335205\n",
      "  time_this_iter_s: 6.644240856170654\n",
      "  time_total_s: 345.63842487335205\n",
      "  timers:\n",
      "    learn_throughput: 1175.639\n",
      "    learn_time_ms: 3402.406\n",
      "    load_throughput: 13006602.062\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 600.676\n",
      "    sample_time_ms: 6659.167\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1665667201\n",
      "  timesteps_since_restore: 208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:20:01,609\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 43470.0x the scale of `vf_clip_param`. This means that it will take more than 43470.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:05 (running for 00:05:58.18)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         345.638</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> -434702</td><td style=\"text-align: right;\">         1.33684e+06</td><td style=\"text-align: right;\">        -1.47235e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-20-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 966497.5810854285\n",
      "  episode_reward_mean: -485146.8951711455\n",
      "  episode_reward_min: -1694634.422840863\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2354\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0367943048477173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015681827440857887\n",
      "          model: {}\n",
      "          policy_loss: -0.00246697966940701\n",
      "          total_loss: 83647324160.0\n",
      "          vf_explained_var: 3.3133774195448495e-06\n",
      "          vf_loss: 83647324160.0\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.566666666666666\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18202179715316788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09000005026556993\n",
      "    mean_inference_ms: 1.0956173008012984\n",
      "    mean_raw_obs_processing_ms: 0.1341505862709345\n",
      "  time_since_restore: 352.2027122974396\n",
      "  time_this_iter_s: 6.564287424087524\n",
      "  time_total_s: 352.2027122974396\n",
      "  timers:\n",
      "    learn_throughput: 1175.919\n",
      "    learn_time_ms: 3401.596\n",
      "    load_throughput: 12692703.889\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 599.732\n",
      "    sample_time_ms: 6669.644\n",
      "    update_time_ms: 2.485\n",
      "  timestamp: 1665667208\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:20:08,221\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 48515.0x the scale of `vf_clip_param`. This means that it will take more than 48515.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:11 (running for 00:06:03.74)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         352.203</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\"> -485147</td><td style=\"text-align: right;\">              966498</td><td style=\"text-align: right;\">        -1.69463e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-20-14\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 966497.5810854285\n",
      "  episode_reward_mean: -487513.5868914073\n",
      "  episode_reward_min: -1694634.422840863\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 2400\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.117628812789917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02153829112648964\n",
      "          model: {}\n",
      "          policy_loss: -0.0034334331285208464\n",
      "          total_loss: 60082700288.0\n",
      "          vf_explained_var: 5.651417723129271e-06\n",
      "          vf_loss: 60082700288.0\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.749999999999998\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1819641478828239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0899845300937038\n",
      "    mean_inference_ms: 1.0956291217272294\n",
      "    mean_raw_obs_processing_ms: 0.13410725686015404\n",
      "  time_since_restore: 358.89123463630676\n",
      "  time_this_iter_s: 6.6885223388671875\n",
      "  time_total_s: 358.89123463630676\n",
      "  timers:\n",
      "    learn_throughput: 1175.22\n",
      "    learn_time_ms: 3403.617\n",
      "    load_throughput: 12338002.647\n",
      "    load_time_ms: 0.324\n",
      "    sample_throughput: 600.115\n",
      "    sample_time_ms: 6665.387\n",
      "    update_time_ms: 2.471\n",
      "  timestamp: 1665667214\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:20:14,955\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 48751.0x the scale of `vf_clip_param`. This means that it will take more than 48751.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:17 (running for 00:06:09.52)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         358.891</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\"> -487514</td><td style=\"text-align: right;\">              966498</td><td style=\"text-align: right;\">        -1.69463e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-20-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 825150.7369870476\n",
      "  episode_reward_mean: -536121.9349967369\n",
      "  episode_reward_min: -1868621.5748707454\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2444\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1251341104507446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009856851771473885\n",
      "          model: {}\n",
      "          policy_loss: -0.007936220616102219\n",
      "          total_loss: 87351238656.0\n",
      "          vf_explained_var: 4.799519956577569e-06\n",
      "          vf_loss: 87351238656.0\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.57777777777778\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18199911665158333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09000116786914009\n",
      "    mean_inference_ms: 1.0958471112437616\n",
      "    mean_raw_obs_processing_ms: 0.13412713891284184\n",
      "  time_since_restore: 365.52873611450195\n",
      "  time_this_iter_s: 6.63750147819519\n",
      "  time_total_s: 365.52873611450195\n",
      "  timers:\n",
      "    learn_throughput: 1174.81\n",
      "    learn_time_ms: 3404.805\n",
      "    load_throughput: 12679274.486\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 601.203\n",
      "    sample_time_ms: 6653.323\n",
      "    update_time_ms: 2.559\n",
      "  timestamp: 1665667221\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:20:21,643\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 53612.0x the scale of `vf_clip_param`. This means that it will take more than 53612.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:22 (running for 00:06:15.17)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         365.529</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -536122</td><td style=\"text-align: right;\">              825151</td><td style=\"text-align: right;\">        -1.86862e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:27 (running for 00:06:20.22)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         365.529</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -536122</td><td style=\"text-align: right;\">              825151</td><td style=\"text-align: right;\">        -1.86862e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-20-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 825150.7369870476\n",
      "  episode_reward_mean: -580971.9582685574\n",
      "  episode_reward_min: -1868621.5748707454\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2488\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1614933013916016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01739305816590786\n",
      "          model: {}\n",
      "          policy_loss: -0.005228159949183464\n",
      "          total_loss: 75723915264.0\n",
      "          vf_explained_var: 6.247079454624327e-06\n",
      "          vf_loss: 75723915264.0\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.71\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18203721534469552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09000632808798507\n",
      "    mean_inference_ms: 1.0958723479863213\n",
      "    mean_raw_obs_processing_ms: 0.13414028301258807\n",
      "  time_since_restore: 372.12751960754395\n",
      "  time_this_iter_s: 6.598783493041992\n",
      "  time_total_s: 372.12751960754395\n",
      "  timers:\n",
      "    learn_throughput: 1174.195\n",
      "    learn_time_ms: 3406.591\n",
      "    load_throughput: 12637252.184\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 601.392\n",
      "    sample_time_ms: 6651.239\n",
      "    update_time_ms: 2.571\n",
      "  timestamp: 1665667228\n",
      "  timesteps_since_restore: 224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:20:28,296\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 58097.0x the scale of `vf_clip_param`. This means that it will take more than 58097.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:33 (running for 00:06:25.82)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         372.128</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\"> -580972</td><td style=\"text-align: right;\">              825151</td><td style=\"text-align: right;\">        -1.86862e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-20-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1012822.0925124971\n",
      "  episode_reward_mean: -544896.3577069455\n",
      "  episode_reward_min: -1868621.5748707454\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2532\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1643227338790894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013965759426355362\n",
      "          model: {}\n",
      "          policy_loss: -0.012562901712954044\n",
      "          total_loss: 68884545536.0\n",
      "          vf_explained_var: 2.043926542683039e-06\n",
      "          vf_loss: 68884545536.0\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.677777777777779\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18199213540597367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08998412891294301\n",
      "    mean_inference_ms: 1.0956316860011406\n",
      "    mean_raw_obs_processing_ms: 0.13409240493860414\n",
      "  time_since_restore: 378.6137773990631\n",
      "  time_this_iter_s: 6.486257791519165\n",
      "  time_total_s: 378.6137773990631\n",
      "  timers:\n",
      "    learn_throughput: 1174.372\n",
      "    learn_time_ms: 3406.075\n",
      "    load_throughput: 12627740.479\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 601.512\n",
      "    sample_time_ms: 6649.907\n",
      "    update_time_ms: 2.538\n",
      "  timestamp: 1665667234\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:20:34,833\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 54490.0x the scale of `vf_clip_param`. This means that it will take more than 54490.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:38 (running for 00:06:31.40)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         378.614</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -544896</td><td style=\"text-align: right;\">         1.01282e+06</td><td style=\"text-align: right;\">        -1.86862e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-20-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1012822.0925124971\n",
      "  episode_reward_mean: -531809.222576232\n",
      "  episode_reward_min: -1524743.9411177675\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2576\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.6075135469436646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009767907671630383\n",
      "          model: {}\n",
      "          policy_loss: -0.005033382214605808\n",
      "          total_loss: 71998849024.0\n",
      "          vf_explained_var: -2.4609028059785487e-06\n",
      "          vf_loss: 71998849024.0\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.290000000000001\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18200387936598234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08999010917210475\n",
      "    mean_inference_ms: 1.095704512820828\n",
      "    mean_raw_obs_processing_ms: 0.134088043150886\n",
      "  time_since_restore: 385.33728981018066\n",
      "  time_this_iter_s: 6.723512411117554\n",
      "  time_total_s: 385.33728981018066\n",
      "  timers:\n",
      "    learn_throughput: 1174.243\n",
      "    learn_time_ms: 3406.451\n",
      "    load_throughput: 13236462.327\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 600.865\n",
      "    sample_time_ms: 6657.072\n",
      "    update_time_ms: 2.539\n",
      "  timestamp: 1665667241\n",
      "  timesteps_since_restore: 232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:20:41,608\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 53181.0x the scale of `vf_clip_param`. This means that it will take more than 53181.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:44 (running for 00:06:37.14)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         385.337</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\"> -531809</td><td style=\"text-align: right;\">         1.01282e+06</td><td style=\"text-align: right;\">        -1.52474e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1012822.0925124971\n",
      "  episode_reward_mean: -595622.8281939421\n",
      "  episode_reward_min: -1540511.692929216\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 2622\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.207789659500122\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01757282018661499\n",
      "          model: {}\n",
      "          policy_loss: -0.0010405597276985645\n",
      "          total_loss: 81656709120.0\n",
      "          vf_explained_var: 2.020853798967437e-06\n",
      "          vf_loss: 81656709120.0\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.155555555555557\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1820255291268304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09001176677361639\n",
      "    mean_inference_ms: 1.0960634128941695\n",
      "    mean_raw_obs_processing_ms: 0.1341034713871002\n",
      "  time_since_restore: 391.94686794281006\n",
      "  time_this_iter_s: 6.6095781326293945\n",
      "  time_total_s: 391.94686794281006\n",
      "  timers:\n",
      "    learn_throughput: 1173.853\n",
      "    learn_time_ms: 3407.582\n",
      "    load_throughput: 13240640.833\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 600.95\n",
      "    sample_time_ms: 6656.126\n",
      "    update_time_ms: 2.556\n",
      "  timestamp: 1665667248\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:20:48,273\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 59562.0x the scale of `vf_clip_param`. This means that it will take more than 59562.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:50 (running for 00:06:42.80)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         391.947</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> -595623</td><td style=\"text-align: right;\">         1.01282e+06</td><td style=\"text-align: right;\">        -1.54051e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-20-55\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 830196.8056895175\n",
      "  episode_reward_mean: -593357.1203556771\n",
      "  episode_reward_min: -1820524.01880311\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2666\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2890551090240479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01434052549302578\n",
      "          model: {}\n",
      "          policy_loss: -0.004734880290925503\n",
      "          total_loss: 74834083840.0\n",
      "          vf_explained_var: 2.175377176172333e-06\n",
      "          vf_loss: 74834083840.0\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.290000000000001\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1820620224663034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0900331297579885\n",
      "    mean_inference_ms: 1.0963656516111653\n",
      "    mean_raw_obs_processing_ms: 0.13412760732471934\n",
      "  time_since_restore: 398.6800854206085\n",
      "  time_this_iter_s: 6.733217477798462\n",
      "  time_total_s: 398.6800854206085\n",
      "  timers:\n",
      "    learn_throughput: 1173.887\n",
      "    learn_time_ms: 3407.484\n",
      "    load_throughput: 14055978.552\n",
      "    load_time_ms: 0.285\n",
      "    sample_throughput: 598.813\n",
      "    sample_time_ms: 6679.877\n",
      "    update_time_ms: 2.581\n",
      "  timestamp: 1665667255\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:20:55,056\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 59336.0x the scale of `vf_clip_param`. This means that it will take more than 59336.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:20:56 (running for 00:06:48.58)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">          398.68</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> -593357</td><td style=\"text-align: right;\">              830197</td><td style=\"text-align: right;\">        -1.82052e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:01 (running for 00:06:53.63)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">          398.68</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> -593357</td><td style=\"text-align: right;\">              830197</td><td style=\"text-align: right;\">        -1.82052e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-21-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 984103.7476243136\n",
      "  episode_reward_mean: -531689.6133265916\n",
      "  episode_reward_min: -1820524.01880311\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2710\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2059966325759888\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013287731446325779\n",
      "          model: {}\n",
      "          policy_loss: -0.006351038347929716\n",
      "          total_loss: 82200051712.0\n",
      "          vf_explained_var: 1.056130258803023e-05\n",
      "          vf_loss: 82200051712.0\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.433333333333334\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18208766611205376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09003988065362703\n",
      "    mean_inference_ms: 1.0964530117763578\n",
      "    mean_raw_obs_processing_ms: 0.1341315704263796\n",
      "  time_since_restore: 405.27637577056885\n",
      "  time_this_iter_s: 6.596290349960327\n",
      "  time_total_s: 405.27637577056885\n",
      "  timers:\n",
      "    learn_throughput: 1173.672\n",
      "    learn_time_ms: 3408.108\n",
      "    load_throughput: 13577094.764\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 598.945\n",
      "    sample_time_ms: 6678.411\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1665667261\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:21:01,698\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 53169.0x the scale of `vf_clip_param`. This means that it will take more than 53169.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:06 (running for 00:06:59.22)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         405.276</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\"> -531690</td><td style=\"text-align: right;\">              984104</td><td style=\"text-align: right;\">        -1.82052e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-21-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 984103.7476243136\n",
      "  episode_reward_mean: -579996.8359643905\n",
      "  episode_reward_min: -1785550.2481373013\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2754\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1066198348999023\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016030043363571167\n",
      "          model: {}\n",
      "          policy_loss: -0.008479689247906208\n",
      "          total_loss: 99155574784.0\n",
      "          vf_explained_var: 5.318849161994876e-06\n",
      "          vf_loss: 99155574784.0\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.469999999999999\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18207861550776833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09003599550725332\n",
      "    mean_inference_ms: 1.0965491482637848\n",
      "    mean_raw_obs_processing_ms: 0.13410648128006614\n",
      "  time_since_restore: 411.8439667224884\n",
      "  time_this_iter_s: 6.567590951919556\n",
      "  time_total_s: 411.8439667224884\n",
      "  timers:\n",
      "    learn_throughput: 1173.111\n",
      "    learn_time_ms: 3409.737\n",
      "    load_throughput: 13036922.838\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 599.852\n",
      "    sample_time_ms: 6668.315\n",
      "    update_time_ms: 2.569\n",
      "  timestamp: 1665667268\n",
      "  timesteps_since_restore: 248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:21:08,309\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 58000.0x the scale of `vf_clip_param`. This means that it will take more than 58000.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:12 (running for 00:07:04.88)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         411.844</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> -579997</td><td style=\"text-align: right;\">              984104</td><td style=\"text-align: right;\">        -1.78555e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-21-14\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 984103.7476243136\n",
      "  episode_reward_mean: -555604.9349826688\n",
      "  episode_reward_min: -1785550.2481373013\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 2800\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8851742148399353\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017166415229439735\n",
      "          model: {}\n",
      "          policy_loss: -0.00907523836940527\n",
      "          total_loss: 64883343360.0\n",
      "          vf_explained_var: 3.2321740945917554e-06\n",
      "          vf_loss: 64883343360.0\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.47777777777778\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1819814470333368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08999966948249949\n",
      "    mean_inference_ms: 1.09632265599561\n",
      "    mean_raw_obs_processing_ms: 0.1340273209551076\n",
      "  time_since_restore: 418.3248710632324\n",
      "  time_this_iter_s: 6.4809043407440186\n",
      "  time_total_s: 418.3248710632324\n",
      "  timers:\n",
      "    learn_throughput: 1172.387\n",
      "    learn_time_ms: 3411.842\n",
      "    load_throughput: 12688864.015\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 600.651\n",
      "    sample_time_ms: 6659.436\n",
      "    update_time_ms: 2.561\n",
      "  timestamp: 1665667274\n",
      "  timesteps_since_restore: 252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:21:14,836\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 55560.0x the scale of `vf_clip_param`. This means that it will take more than 55560.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:17 (running for 00:07:10.36)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         418.325</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> -555605</td><td style=\"text-align: right;\">              984104</td><td style=\"text-align: right;\">        -1.78555e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-21-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1105894.0825544058\n",
      "  episode_reward_mean: -516362.17931166984\n",
      "  episode_reward_min: -1785550.2481373013\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2844\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9176168441772461\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025124115869402885\n",
      "          model: {}\n",
      "          policy_loss: -0.003649882273748517\n",
      "          total_loss: 89672343552.0\n",
      "          vf_explained_var: 5.31500381839578e-06\n",
      "          vf_loss: 89672343552.0\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.499999999999998\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18197682064149315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09000003952323983\n",
      "    mean_inference_ms: 1.0963846285834415\n",
      "    mean_raw_obs_processing_ms: 0.13402451993445233\n",
      "  time_since_restore: 425.0796227455139\n",
      "  time_this_iter_s: 6.754751682281494\n",
      "  time_total_s: 425.0796227455139\n",
      "  timers:\n",
      "    learn_throughput: 1172.268\n",
      "    learn_time_ms: 3412.188\n",
      "    load_throughput: 13304691.515\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 599.865\n",
      "    sample_time_ms: 6668.163\n",
      "    update_time_ms: 2.568\n",
      "  timestamp: 1665667281\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:21:21,640\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 51636.0x the scale of `vf_clip_param`. This means that it will take more than 51636.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:23 (running for 00:07:16.16)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">          425.08</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\"> -516362</td><td style=\"text-align: right;\">         1.10589e+06</td><td style=\"text-align: right;\">        -1.78555e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1105894.0825544058\n",
      "  episode_reward_mean: -477865.6850947815\n",
      "  episode_reward_min: -1540724.123408879\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2888\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0381224155426025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010573449544608593\n",
      "          model: {}\n",
      "          policy_loss: -0.010063407011330128\n",
      "          total_loss: 67597963264.0\n",
      "          vf_explained_var: 5.589056854660157e-06\n",
      "          vf_loss: 67597963264.0\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.277777777777779\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18205826283096205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09003187534080659\n",
      "    mean_inference_ms: 1.0967248991146965\n",
      "    mean_raw_obs_processing_ms: 0.13408010723270564\n",
      "  time_since_restore: 431.7359082698822\n",
      "  time_this_iter_s: 6.656285524368286\n",
      "  time_total_s: 431.7359082698822\n",
      "  timers:\n",
      "    learn_throughput: 1171.846\n",
      "    learn_time_ms: 3413.418\n",
      "    load_throughput: 13359783.405\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 599.831\n",
      "    sample_time_ms: 6668.542\n",
      "    update_time_ms: 2.513\n",
      "  timestamp: 1665667288\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:21:28,340\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 47787.0x the scale of `vf_clip_param`. This means that it will take more than 47787.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:29 (running for 00:07:21.91)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         431.736</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -477866</td><td style=\"text-align: right;\">         1.10589e+06</td><td style=\"text-align: right;\">        -1.54072e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:34 (running for 00:07:26.91)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         431.736</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -477866</td><td style=\"text-align: right;\">         1.10589e+06</td><td style=\"text-align: right;\">        -1.54072e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-21-35\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1115896.316441658\n",
      "  episode_reward_mean: -535332.5325715758\n",
      "  episode_reward_min: -1625265.2839778448\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2932\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2997500896453857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02243298850953579\n",
      "          model: {}\n",
      "          policy_loss: -0.004753562621772289\n",
      "          total_loss: 90247921664.0\n",
      "          vf_explained_var: 1.4096113773121033e-05\n",
      "          vf_loss: 90247921664.0\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.59\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18213174143180413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09006907443841933\n",
      "    mean_inference_ms: 1.097181855881749\n",
      "    mean_raw_obs_processing_ms: 0.13413532682689847\n",
      "  time_since_restore: 438.4247853755951\n",
      "  time_this_iter_s: 6.688877105712891\n",
      "  time_total_s: 438.4247853755951\n",
      "  timers:\n",
      "    learn_throughput: 1171.92\n",
      "    learn_time_ms: 3413.204\n",
      "    load_throughput: 13378960.128\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 598.927\n",
      "    sample_time_ms: 6678.608\n",
      "    update_time_ms: 2.602\n",
      "  timestamp: 1665667295\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:21:35,081\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 53533.0x the scale of `vf_clip_param`. This means that it will take more than 53533.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:40 (running for 00:07:32.65)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         438.425</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\"> -535333</td><td style=\"text-align: right;\">          1.1159e+06</td><td style=\"text-align: right;\">        -1.62527e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1115896.316441658\n",
      "  episode_reward_mean: -474703.15350136446\n",
      "  episode_reward_min: -1633693.3198181696\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 2976\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7796475887298584\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01428838912397623\n",
      "          model: {}\n",
      "          policy_loss: -0.008831292390823364\n",
      "          total_loss: 59910815744.0\n",
      "          vf_explained_var: 5.031080490880413e-06\n",
      "          vf_loss: 59910815744.0\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.677777777777777\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18216639398095302\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0900869821909923\n",
      "    mean_inference_ms: 1.0974127530657112\n",
      "    mean_raw_obs_processing_ms: 0.13415379182547352\n",
      "  time_since_restore: 445.0012345314026\n",
      "  time_this_iter_s: 6.576449155807495\n",
      "  time_total_s: 445.0012345314026\n",
      "  timers:\n",
      "    learn_throughput: 1170.874\n",
      "    learn_time_ms: 3416.251\n",
      "    load_throughput: 13284674.954\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 598.386\n",
      "    sample_time_ms: 6684.65\n",
      "    update_time_ms: 2.588\n",
      "  timestamp: 1665667301\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:21:41,710\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 47470.0x the scale of `vf_clip_param`. This means that it will take more than 47470.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:45 (running for 00:07:38.24)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         445.001</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\"> -474703</td><td style=\"text-align: right;\">          1.1159e+06</td><td style=\"text-align: right;\">        -1.63369e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-21-48\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 672967.2513324069\n",
      "  episode_reward_mean: -437014.3625570698\n",
      "  episode_reward_min: -1633693.3198181696\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 3022\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4376715421676636\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014110737480223179\n",
      "          model: {}\n",
      "          policy_loss: -0.006686493754386902\n",
      "          total_loss: 44533686272.0\n",
      "          vf_explained_var: 1.2170755326224025e-05\n",
      "          vf_loss: 44533686272.0\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.18\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18208932089859872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09005986815685568\n",
      "    mean_inference_ms: 1.0972254317600842\n",
      "    mean_raw_obs_processing_ms: 0.13409328511478613\n",
      "  time_since_restore: 451.61231756210327\n",
      "  time_this_iter_s: 6.611083030700684\n",
      "  time_total_s: 451.61231756210327\n",
      "  timers:\n",
      "    learn_throughput: 1171.278\n",
      "    learn_time_ms: 3415.074\n",
      "    load_throughput: 13677821.621\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 599.036\n",
      "    sample_time_ms: 6677.391\n",
      "    update_time_ms: 2.547\n",
      "  timestamp: 1665667308\n",
      "  timesteps_since_restore: 272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:21:48,369\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 43701.0x the scale of `vf_clip_param`. This means that it will take more than 43701.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:51 (running for 00:07:43.92)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         451.612</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\"> -437014</td><td style=\"text-align: right;\">              672967</td><td style=\"text-align: right;\">        -1.63369e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-21-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 672967.2513324069\n",
      "  episode_reward_mean: -472529.90775922616\n",
      "  episode_reward_min: -1507362.2932199666\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3066\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.915173351764679\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008448977023363113\n",
      "          model: {}\n",
      "          policy_loss: -0.006682473234832287\n",
      "          total_loss: 62446825472.0\n",
      "          vf_explained_var: -2.8833906071668025e-06\n",
      "          vf_loss: 62446825472.0\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.233333333333334\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18202048559545536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.090022749514922\n",
      "    mean_inference_ms: 1.0968435827672238\n",
      "    mean_raw_obs_processing_ms: 0.13403458773125934\n",
      "  time_since_restore: 458.18661975860596\n",
      "  time_this_iter_s: 6.5743021965026855\n",
      "  time_total_s: 458.18661975860596\n",
      "  timers:\n",
      "    learn_throughput: 1171.387\n",
      "    learn_time_ms: 3414.754\n",
      "    load_throughput: 13672248.391\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 599.516\n",
      "    sample_time_ms: 6672.046\n",
      "    update_time_ms: 2.512\n",
      "  timestamp: 1665667314\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:21:54,989\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 47253.0x the scale of `vf_clip_param`. This means that it will take more than 47253.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:21:57 (running for 00:07:49.51)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         458.187</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> -472530</td><td style=\"text-align: right;\">              672967</td><td style=\"text-align: right;\">        -1.50736e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-22-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 672967.2513324069\n",
      "  episode_reward_mean: -504490.126090258\n",
      "  episode_reward_min: -1670113.9996936051\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3110\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7134535908699036\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0109794232994318\n",
      "          model: {}\n",
      "          policy_loss: 0.0016146654961630702\n",
      "          total_loss: 66326097920.0\n",
      "          vf_explained_var: 8.6325471784221e-06\n",
      "          vf_loss: 66326097920.0\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.21\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.181945630828167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08997224755728116\n",
      "    mean_inference_ms: 1.0962168898810587\n",
      "    mean_raw_obs_processing_ms: 0.13397194506574756\n",
      "  time_since_restore: 464.67478036880493\n",
      "  time_this_iter_s: 6.488160610198975\n",
      "  time_total_s: 464.67478036880493\n",
      "  timers:\n",
      "    learn_throughput: 1171.55\n",
      "    learn_time_ms: 3414.281\n",
      "    load_throughput: 13539840.207\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 601.759\n",
      "    sample_time_ms: 6647.181\n",
      "    update_time_ms: 2.519\n",
      "  timestamp: 1665667321\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:22:01,523\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 50449.0x the scale of `vf_clip_param`. This means that it will take more than 50449.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:02 (running for 00:07:55.05)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         464.675</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> -504490</td><td style=\"text-align: right;\">              672967</td><td style=\"text-align: right;\">        -1.67011e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:07 (running for 00:08:00.06)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         464.675</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> -504490</td><td style=\"text-align: right;\">              672967</td><td style=\"text-align: right;\">        -1.67011e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 946908.7084426604\n",
      "  episode_reward_mean: -449775.8131674831\n",
      "  episode_reward_min: -1704895.680342316\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3154\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47033238410949707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015048080123960972\n",
      "          model: {}\n",
      "          policy_loss: -0.004600726533681154\n",
      "          total_loss: 75323441152.0\n",
      "          vf_explained_var: 1.7961956473300233e-05\n",
      "          vf_loss: 75323441152.0\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.411111111111111\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18192875315186807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08996490485664733\n",
      "    mean_inference_ms: 1.0961860539388129\n",
      "    mean_raw_obs_processing_ms: 0.13395905285629595\n",
      "  time_since_restore: 471.4193661212921\n",
      "  time_this_iter_s: 6.744585752487183\n",
      "  time_total_s: 471.4193661212921\n",
      "  timers:\n",
      "    learn_throughput: 1171.796\n",
      "    learn_time_ms: 3413.564\n",
      "    load_throughput: 13860885.658\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 600.332\n",
      "    sample_time_ms: 6662.984\n",
      "    update_time_ms: 2.587\n",
      "  timestamp: 1665667328\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:22:08,321\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 44978.0x the scale of `vf_clip_param`. This means that it will take more than 44978.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:13 (running for 00:08:05.89)<br>Memory usage on this node: 10.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         471.419</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\"> -449776</td><td style=\"text-align: right;\">              946909</td><td style=\"text-align: right;\">         -1.7049e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-22-14\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 946908.7084426604\n",
      "  episode_reward_mean: -514111.80455832893\n",
      "  episode_reward_min: -1704895.680342316\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 3200\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.529456377029419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013243738561868668\n",
      "          model: {}\n",
      "          policy_loss: -0.004769091960042715\n",
      "          total_loss: 80617676800.0\n",
      "          vf_explained_var: -6.681123340968043e-05\n",
      "          vf_loss: 80617676800.0\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.780000000000001\n",
      "    ram_util_percent: 17.5\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1819001712438135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08996334714812258\n",
      "    mean_inference_ms: 1.096287066278419\n",
      "    mean_raw_obs_processing_ms: 0.13393099878925674\n",
      "  time_since_restore: 477.9321537017822\n",
      "  time_this_iter_s: 6.512787580490112\n",
      "  time_total_s: 477.9321537017822\n",
      "  timers:\n",
      "    learn_throughput: 1173.093\n",
      "    learn_time_ms: 3409.789\n",
      "    load_throughput: 13910302.628\n",
      "    load_time_ms: 0.288\n",
      "    sample_throughput: 600.47\n",
      "    sample_time_ms: 6661.45\n",
      "    update_time_ms: 2.532\n",
      "  timestamp: 1665667334\n",
      "  timesteps_since_restore: 288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:22:14,886\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 51411.0x the scale of `vf_clip_param`. This means that it will take more than 51411.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:18 (running for 00:08:11.42)<br>Memory usage on this node: 10.8/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         477.932</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\"> -514112</td><td style=\"text-align: right;\">              946909</td><td style=\"text-align: right;\">         -1.7049e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-22-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1239766.4706025822\n",
      "  episode_reward_mean: -552961.5956841876\n",
      "  episode_reward_min: -1638227.0197801918\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3244\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1020840406417847\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0178117286413908\n",
      "          model: {}\n",
      "          policy_loss: 0.005124968476593494\n",
      "          total_loss: 90030522368.0\n",
      "          vf_explained_var: 3.865265171043575e-06\n",
      "          vf_loss: 90030522368.0\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.844444444444443\n",
      "    ram_util_percent: 17.366666666666664\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18175666654529155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08989088411633593\n",
      "    mean_inference_ms: 1.095498444139002\n",
      "    mean_raw_obs_processing_ms: 0.13380833348065288\n",
      "  time_since_restore: 484.3459234237671\n",
      "  time_this_iter_s: 6.413769721984863\n",
      "  time_total_s: 484.3459234237671\n",
      "  timers:\n",
      "    learn_throughput: 1173.447\n",
      "    learn_time_ms: 3408.762\n",
      "    load_throughput: 13921845.49\n",
      "    load_time_ms: 0.287\n",
      "    sample_throughput: 601.268\n",
      "    sample_time_ms: 6652.609\n",
      "    update_time_ms: 2.464\n",
      "  timestamp: 1665667341\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:22:21,352\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 55296.0x the scale of `vf_clip_param`. This means that it will take more than 55296.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:24 (running for 00:08:16.91)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         484.346</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\"> -552962</td><td style=\"text-align: right;\">         1.23977e+06</td><td style=\"text-align: right;\">        -1.63823e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-22-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1239766.4706025822\n",
      "  episode_reward_mean: -505353.7029097623\n",
      "  episode_reward_min: -1718532.1357202767\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3288\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9015757441520691\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0059952642768621445\n",
      "          model: {}\n",
      "          policy_loss: -0.001772459945641458\n",
      "          total_loss: 69502296064.0\n",
      "          vf_explained_var: 3.743428169400431e-05\n",
      "          vf_loss: 69502296064.0\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.42\n",
      "    ram_util_percent: 17.199999999999996\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18168201607953058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984075357029664\n",
      "    mean_inference_ms: 1.0948761144477654\n",
      "    mean_raw_obs_processing_ms: 0.1337408655186858\n",
      "  time_since_restore: 491.01160740852356\n",
      "  time_this_iter_s: 6.66568398475647\n",
      "  time_total_s: 491.01160740852356\n",
      "  timers:\n",
      "    learn_throughput: 1173.764\n",
      "    learn_time_ms: 3407.841\n",
      "    load_throughput: 13224985.023\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 602.163\n",
      "    sample_time_ms: 6642.725\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1665667348\n",
      "  timesteps_since_restore: 296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:22:28,058\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 50535.0x the scale of `vf_clip_param`. This means that it will take more than 50535.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:30 (running for 00:08:22.58)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         491.012</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -505354</td><td style=\"text-align: right;\">         1.23977e+06</td><td style=\"text-align: right;\">        -1.71853e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:22:34,801\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 52484.0x the scale of `vf_clip_param`. This means that it will take more than 52484.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-22-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 934669.9519595274\n",
      "  episode_reward_mean: -524838.6267912296\n",
      "  episode_reward_min: -1814776.5134072916\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3332\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9053293466567993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013315346091985703\n",
      "          model: {}\n",
      "          policy_loss: -0.0020958257373422384\n",
      "          total_loss: 88953421824.0\n",
      "          vf_explained_var: 1.705237627902534e-05\n",
      "          vf_loss: 88953421824.0\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.511111111111113\n",
      "    ram_util_percent: 17.2\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1816843320905825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984283542150512\n",
      "    mean_inference_ms: 1.094961888610848\n",
      "    mean_raw_obs_processing_ms: 0.13374043071396874\n",
      "  time_since_restore: 497.71242904663086\n",
      "  time_this_iter_s: 6.7008216381073\n",
      "  time_total_s: 497.71242904663086\n",
      "  timers:\n",
      "    learn_throughput: 1174.208\n",
      "    learn_time_ms: 3406.552\n",
      "    load_throughput: 12822696.423\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 601.747\n",
      "    sample_time_ms: 6647.308\n",
      "    update_time_ms: 2.503\n",
      "  timestamp: 1665667354\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:35 (running for 00:08:28.33)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         497.712</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\"> -524839</td><td style=\"text-align: right;\">              934670</td><td style=\"text-align: right;\">        -1.81478e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:40 (running for 00:08:33.34)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         497.712</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\"> -524839</td><td style=\"text-align: right;\">              934670</td><td style=\"text-align: right;\">        -1.81478e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-22-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 934669.9519595274\n",
      "  episode_reward_mean: -557881.7441946544\n",
      "  episode_reward_min: -1860988.1373872138\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3376\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0301275253295898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0099044069647789\n",
      "          model: {}\n",
      "          policy_loss: -0.010783226229250431\n",
      "          total_loss: 57529217024.0\n",
      "          vf_explained_var: -1.7415451338820276e-06\n",
      "          vf_loss: 57529217024.0\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.48\n",
      "    ram_util_percent: 17.199999999999996\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1817045476870289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985541602254553\n",
      "    mean_inference_ms: 1.0951629748670972\n",
      "    mean_raw_obs_processing_ms: 0.13376365443009422\n",
      "  time_since_restore: 504.3354573249817\n",
      "  time_this_iter_s: 6.62302827835083\n",
      "  time_total_s: 504.3354573249817\n",
      "  timers:\n",
      "    learn_throughput: 1174.008\n",
      "    learn_time_ms: 3407.132\n",
      "    load_throughput: 12844293.37\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 602.522\n",
      "    sample_time_ms: 6638.763\n",
      "    update_time_ms: 2.476\n",
      "  timestamp: 1665667361\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:22:41,474\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 55788.0x the scale of `vf_clip_param`. This means that it will take more than 55788.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:46 (running for 00:08:39.04)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         504.335</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\"> -557882</td><td style=\"text-align: right;\">              934670</td><td style=\"text-align: right;\">        -1.86099e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-22-48\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 728354.7453711343\n",
      "  episode_reward_mean: -581060.2407732703\n",
      "  episode_reward_min: -1860988.1373872138\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 3422\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.139062523841858\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.110644817352295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02087615244090557\n",
      "          model: {}\n",
      "          policy_loss: 0.0009313324699178338\n",
      "          total_loss: 87423410176.0\n",
      "          vf_explained_var: 1.0713005394791253e-05\n",
      "          vf_loss: 87423410176.0\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.422222222222224\n",
      "    ram_util_percent: 17.2\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18165573953092795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984385043328659\n",
      "    mean_inference_ms: 1.0951125802768746\n",
      "    mean_raw_obs_processing_ms: 0.13372607965473082\n",
      "  time_since_restore: 510.8478012084961\n",
      "  time_this_iter_s: 6.512343883514404\n",
      "  time_total_s: 510.8478012084961\n",
      "  timers:\n",
      "    learn_throughput: 1174.271\n",
      "    learn_time_ms: 3406.367\n",
      "    load_throughput: 12919464.038\n",
      "    load_time_ms: 0.31\n",
      "    sample_throughput: 602.981\n",
      "    sample_time_ms: 6633.711\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1665667368\n",
      "  timesteps_since_restore: 308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:22:48,040\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 58106.0x the scale of `vf_clip_param`. This means that it will take more than 58106.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:52 (running for 00:08:44.57)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         510.848</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\"> -581060</td><td style=\"text-align: right;\">              728355</td><td style=\"text-align: right;\">        -1.86099e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-22-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 748857.619654732\n",
      "  episode_reward_mean: -579042.0731557115\n",
      "  episode_reward_min: -1640692.1948541172\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3466\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.991172194480896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010824271477758884\n",
      "          model: {}\n",
      "          policy_loss: -0.0005893716006539762\n",
      "          total_loss: 83521560576.0\n",
      "          vf_explained_var: 9.5621871878393e-06\n",
      "          vf_loss: 83521560576.0\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.35\n",
      "    ram_util_percent: 17.199999999999996\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18162644600698044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0898311752588995\n",
      "    mean_inference_ms: 1.0949788254926356\n",
      "    mean_raw_obs_processing_ms: 0.13369612494738456\n",
      "  time_since_restore: 517.4663453102112\n",
      "  time_this_iter_s: 6.618544101715088\n",
      "  time_total_s: 517.4663453102112\n",
      "  timers:\n",
      "    learn_throughput: 1174.144\n",
      "    learn_time_ms: 3406.738\n",
      "    load_throughput: 12565320.551\n",
      "    load_time_ms: 0.318\n",
      "    sample_throughput: 602.962\n",
      "    sample_time_ms: 6633.919\n",
      "    update_time_ms: 2.521\n",
      "  timestamp: 1665667374\n",
      "  timesteps_since_restore: 312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:22:54,713\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 57904.0x the scale of `vf_clip_param`. This means that it will take more than 57904.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:22:57 (running for 00:08:50.24)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         517.466</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> -579042</td><td style=\"text-align: right;\">              748858</td><td style=\"text-align: right;\">        -1.64069e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-23-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 748857.619654732\n",
      "  episode_reward_mean: -601808.8622326199\n",
      "  episode_reward_min: -1639369.6353479472\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3510\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1159876585006714\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005754163023084402\n",
      "          model: {}\n",
      "          policy_loss: -0.0038214719388633966\n",
      "          total_loss: 93680508928.0\n",
      "          vf_explained_var: 1.5639490129615297e-06\n",
      "          vf_loss: 93680508928.0\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.122222222222224\n",
      "    ram_util_percent: 17.144444444444442\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18170071439347324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985928080277429\n",
      "    mean_inference_ms: 1.0952288527295324\n",
      "    mean_raw_obs_processing_ms: 0.13375479918274125\n",
      "  time_since_restore: 524.2222905158997\n",
      "  time_this_iter_s: 6.755945205688477\n",
      "  time_total_s: 524.2222905158997\n",
      "  timers:\n",
      "    learn_throughput: 1173.61\n",
      "    learn_time_ms: 3408.288\n",
      "    load_throughput: 12552159.21\n",
      "    load_time_ms: 0.319\n",
      "    sample_throughput: 601.379\n",
      "    sample_time_ms: 6651.384\n",
      "    update_time_ms: 2.547\n",
      "  timestamp: 1665667381\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:23:01,520\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 60181.0x the scale of `vf_clip_param`. This means that it will take more than 60181.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:03 (running for 00:08:56.05)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         524.222</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\"> -601809</td><td style=\"text-align: right;\">              748858</td><td style=\"text-align: right;\">        -1.63937e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-23-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 748857.619654732\n",
      "  episode_reward_mean: -583345.9136153266\n",
      "  episode_reward_min: -1493137.0568961382\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3554\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.31187310814857483\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007033149246126413\n",
      "          model: {}\n",
      "          policy_loss: -0.005243247840553522\n",
      "          total_loss: 83349602304.0\n",
      "          vf_explained_var: 1.8228125782115967e-06\n",
      "          vf_loss: 83349602304.0\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.459999999999999\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1817554353305894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08988900806592198\n",
      "    mean_inference_ms: 1.0955548507277553\n",
      "    mean_raw_obs_processing_ms: 0.1338036358639954\n",
      "  time_since_restore: 530.9411406517029\n",
      "  time_this_iter_s: 6.718850135803223\n",
      "  time_total_s: 530.9411406517029\n",
      "  timers:\n",
      "    learn_throughput: 1172.781\n",
      "    learn_time_ms: 3410.698\n",
      "    load_throughput: 11981158.323\n",
      "    load_time_ms: 0.334\n",
      "    sample_throughput: 599.325\n",
      "    sample_time_ms: 6674.179\n",
      "    update_time_ms: 2.573\n",
      "  timestamp: 1665667388\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:23:08,290\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 58335.0x the scale of `vf_clip_param`. This means that it will take more than 58335.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:09 (running for 00:09:01.81)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         530.941</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> -583346</td><td style=\"text-align: right;\">              748858</td><td style=\"text-align: right;\">        -1.49314e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:14 (running for 00:09:06.82)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         530.941</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> -583346</td><td style=\"text-align: right;\">              748858</td><td style=\"text-align: right;\">        -1.49314e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-23-14\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 605984.0037994388\n",
      "  episode_reward_mean: -497460.1121261639\n",
      "  episode_reward_min: -1554589.0907236598\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 3600\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7631836533546448\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012398798018693924\n",
      "          model: {}\n",
      "          policy_loss: 0.0030641104094684124\n",
      "          total_loss: 50658566144.0\n",
      "          vf_explained_var: -2.588463030406274e-05\n",
      "          vf_loss: 50658566144.0\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.011111111111111\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1817419914593868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08989427892297076\n",
      "    mean_inference_ms: 1.0957777448039034\n",
      "    mean_raw_obs_processing_ms: 0.13380091821292073\n",
      "  time_since_restore: 537.5376856327057\n",
      "  time_this_iter_s: 6.596544981002808\n",
      "  time_total_s: 537.5376856327057\n",
      "  timers:\n",
      "    learn_throughput: 1172.106\n",
      "    learn_time_ms: 3412.662\n",
      "    load_throughput: 12012039.808\n",
      "    load_time_ms: 0.333\n",
      "    sample_throughput: 600.673\n",
      "    sample_time_ms: 6659.2\n",
      "    update_time_ms: 2.565\n",
      "  timestamp: 1665667394\n",
      "  timesteps_since_restore: 324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:23:14,932\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 49746.0x the scale of `vf_clip_param`. This means that it will take more than 49746.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:20 (running for 00:09:12.50)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         537.538</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\"> -497460</td><td style=\"text-align: right;\">              605984</td><td style=\"text-align: right;\">        -1.55459e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-23-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 532704.2007866233\n",
      "  episode_reward_mean: -544562.2151679571\n",
      "  episode_reward_min: -1763869.0381827208\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3644\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8151063919067383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0052625141106545925\n",
      "          model: {}\n",
      "          policy_loss: -0.002598509658128023\n",
      "          total_loss: 90735026176.0\n",
      "          vf_explained_var: 5.607643288385589e-06\n",
      "          vf_loss: 90735026176.0\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.499999999999998\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18170620926597686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08987374199529823\n",
      "    mean_inference_ms: 1.0956581548402395\n",
      "    mean_raw_obs_processing_ms: 0.13377117984101028\n",
      "  time_since_restore: 544.1103193759918\n",
      "  time_this_iter_s: 6.572633743286133\n",
      "  time_total_s: 544.1103193759918\n",
      "  timers:\n",
      "    learn_throughput: 1171.022\n",
      "    learn_time_ms: 3415.821\n",
      "    load_throughput: 11994863.802\n",
      "    load_time_ms: 0.333\n",
      "    sample_throughput: 600.29\n",
      "    sample_time_ms: 6663.441\n",
      "    update_time_ms: 2.614\n",
      "  timestamp: 1665667401\n",
      "  timesteps_since_restore: 328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:23:21,552\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 54456.0x the scale of `vf_clip_param`. This means that it will take more than 54456.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:25 (running for 00:09:18.12)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">          544.11</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\"> -544562</td><td style=\"text-align: right;\">              532704</td><td style=\"text-align: right;\">        -1.76387e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-23-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 303262.3029853933\n",
      "  episode_reward_mean: -580304.1721132899\n",
      "  episode_reward_min: -1763869.0381827208\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3688\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9184696078300476\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006842876318842173\n",
      "          model: {}\n",
      "          policy_loss: -0.002246872056275606\n",
      "          total_loss: 62170796032.0\n",
      "          vf_explained_var: 3.7024738048785366e-06\n",
      "          vf_loss: 62170796032.0\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.133333333333333\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18168689285997222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08985060242293123\n",
      "    mean_inference_ms: 1.095347125017398\n",
      "    mean_raw_obs_processing_ms: 0.13374938778779052\n",
      "  time_since_restore: 550.6935064792633\n",
      "  time_this_iter_s: 6.583187103271484\n",
      "  time_total_s: 550.6935064792633\n",
      "  timers:\n",
      "    learn_throughput: 1170.718\n",
      "    learn_time_ms: 3416.708\n",
      "    load_throughput: 12210491.994\n",
      "    load_time_ms: 0.328\n",
      "    sample_throughput: 598.593\n",
      "    sample_time_ms: 6682.337\n",
      "    update_time_ms: 2.617\n",
      "  timestamp: 1665667408\n",
      "  timesteps_since_restore: 332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:23:28,183\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 58030.0x the scale of `vf_clip_param`. This means that it will take more than 58030.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:31 (running for 00:09:23.71)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         550.694</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> -580304</td><td style=\"text-align: right;\">              303262</td><td style=\"text-align: right;\">        -1.76387e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:23:34,761\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 52526.0x the scale of `vf_clip_param`. This means that it will take more than 52526.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-23-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 734401.7952542793\n",
      "  episode_reward_mean: -525258.7017775074\n",
      "  episode_reward_min: -1620026.4066235588\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3732\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8484278321266174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007743925787508488\n",
      "          model: {}\n",
      "          policy_loss: -0.0013584318803623319\n",
      "          total_loss: 60952186880.0\n",
      "          vf_explained_var: 2.8683421987807378e-05\n",
      "          vf_loss: 60952186880.0\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.16\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1816261883548153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08982033657512158\n",
      "    mean_inference_ms: 1.0950251147462502\n",
      "    mean_raw_obs_processing_ms: 0.13370966318808722\n",
      "  time_since_restore: 557.2210416793823\n",
      "  time_this_iter_s: 6.5275352001190186\n",
      "  time_total_s: 557.2210416793823\n",
      "  timers:\n",
      "    learn_throughput: 1170.437\n",
      "    learn_time_ms: 3417.526\n",
      "    load_throughput: 12217605.593\n",
      "    load_time_ms: 0.327\n",
      "    sample_throughput: 599.748\n",
      "    sample_time_ms: 6669.463\n",
      "    update_time_ms: 2.611\n",
      "  timestamp: 1665667414\n",
      "  timesteps_since_restore: 336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:36 (running for 00:09:29.32)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         557.221</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> -525259</td><td style=\"text-align: right;\">              734402</td><td style=\"text-align: right;\">        -1.62003e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-23-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 941140.5578775547\n",
      "  episode_reward_mean: -452823.9872967374\n",
      "  episode_reward_min: -1625557.7217381361\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3776\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7847670912742615\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007242908235639334\n",
      "          model: {}\n",
      "          policy_loss: -0.008410214446485043\n",
      "          total_loss: 60087242752.0\n",
      "          vf_explained_var: 6.769357241864782e-06\n",
      "          vf_loss: 60087242752.0\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.488888888888889\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1815667965727078\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08978964647551688\n",
      "    mean_inference_ms: 1.0947369029717737\n",
      "    mean_raw_obs_processing_ms: 0.13366728253990973\n",
      "  time_since_restore: 563.8376770019531\n",
      "  time_this_iter_s: 6.616635322570801\n",
      "  time_total_s: 563.8376770019531\n",
      "  timers:\n",
      "    learn_throughput: 1170.493\n",
      "    learn_time_ms: 3417.364\n",
      "    load_throughput: 12687904.409\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 600.334\n",
      "    sample_time_ms: 6662.962\n",
      "    update_time_ms: 2.594\n",
      "  timestamp: 1665667421\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:23:41,429\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 45282.0x the scale of `vf_clip_param`. This means that it will take more than 45282.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:42 (running for 00:09:34.96)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         563.838</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\"> -452824</td><td style=\"text-align: right;\">              941141</td><td style=\"text-align: right;\">        -1.62556e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:47 (running for 00:09:40.00)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         563.838</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\"> -452824</td><td style=\"text-align: right;\">              941141</td><td style=\"text-align: right;\">        -1.62556e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-23-48\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 941140.5578775547\n",
      "  episode_reward_mean: -456785.1846327894\n",
      "  episode_reward_min: -1625557.7217381361\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 3822\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7839593291282654\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006309488322585821\n",
      "          model: {}\n",
      "          policy_loss: -0.004098461475223303\n",
      "          total_loss: 69421989888.0\n",
      "          vf_explained_var: 3.209190981579013e-05\n",
      "          vf_loss: 69421989888.0\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.260000000000002\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18150352762624347\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08977133188701643\n",
      "    mean_inference_ms: 1.0946772213428246\n",
      "    mean_raw_obs_processing_ms: 0.1336223032984362\n",
      "  time_since_restore: 570.51540350914\n",
      "  time_this_iter_s: 6.67772650718689\n",
      "  time_total_s: 570.51540350914\n",
      "  timers:\n",
      "    learn_throughput: 1170.542\n",
      "    learn_time_ms: 3417.221\n",
      "    load_throughput: 12950379.004\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 599.81\n",
      "    sample_time_ms: 6668.779\n",
      "    update_time_ms: 2.619\n",
      "  timestamp: 1665667428\n",
      "  timesteps_since_restore: 344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:23:48,161\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 45679.0x the scale of `vf_clip_param`. This means that it will take more than 45679.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:53 (running for 00:09:45.69)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         570.515</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> -456785</td><td style=\"text-align: right;\">              941141</td><td style=\"text-align: right;\">        -1.62556e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-23-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 543533.3175955022\n",
      "  episode_reward_mean: -463563.59846403025\n",
      "  episode_reward_min: -1522018.8341546098\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3866\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7072375416755676\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0073422943241894245\n",
      "          model: {}\n",
      "          policy_loss: -0.0004699081473518163\n",
      "          total_loss: 49518637056.0\n",
      "          vf_explained_var: 8.041755791055039e-06\n",
      "          vf_loss: 49518637056.0\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.088888888888889\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18151225651106365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08977606714694637\n",
      "    mean_inference_ms: 1.0947998474734322\n",
      "    mean_raw_obs_processing_ms: 0.13362793749419283\n",
      "  time_since_restore: 577.2166328430176\n",
      "  time_this_iter_s: 6.7012293338775635\n",
      "  time_total_s: 577.2166328430176\n",
      "  timers:\n",
      "    learn_throughput: 1170.672\n",
      "    learn_time_ms: 3416.842\n",
      "    load_throughput: 12974414.972\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 598.109\n",
      "    sample_time_ms: 6687.748\n",
      "    update_time_ms: 2.667\n",
      "  timestamp: 1665667434\n",
      "  timesteps_since_restore: 348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:23:54,915\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 46356.0x the scale of `vf_clip_param`. This means that it will take more than 46356.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:23:59 (running for 00:09:51.48)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         577.217</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\"> -463564</td><td style=\"text-align: right;\">              543533</td><td style=\"text-align: right;\">        -1.52202e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-24-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 543533.3175955022\n",
      "  episode_reward_mean: -468519.070406697\n",
      "  episode_reward_min: -1415769.264770692\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3910\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0874221324920654\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006509528029710054\n",
      "          model: {}\n",
      "          policy_loss: -0.008173041045665741\n",
      "          total_loss: 67527688192.0\n",
      "          vf_explained_var: 4.246991011314094e-06\n",
      "          vf_loss: 67527688192.0\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.69\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18157716149232647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08979417986987698\n",
      "    mean_inference_ms: 1.0949930439333309\n",
      "    mean_raw_obs_processing_ms: 0.1336708061625693\n",
      "  time_since_restore: 583.9397656917572\n",
      "  time_this_iter_s: 6.723132848739624\n",
      "  time_total_s: 583.9397656917572\n",
      "  timers:\n",
      "    learn_throughput: 1170.405\n",
      "    learn_time_ms: 3417.621\n",
      "    load_throughput: 12943385.28\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 597.336\n",
      "    sample_time_ms: 6696.396\n",
      "    update_time_ms: 2.627\n",
      "  timestamp: 1665667441\n",
      "  timesteps_since_restore: 352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:24:01,684\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 46852.0x the scale of `vf_clip_param`. This means that it will take more than 46852.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:24:04 (running for 00:09:57.20)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">          583.94</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\"> -468519</td><td style=\"text-align: right;\">              543533</td><td style=\"text-align: right;\">        -1.41577e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-24-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 364402.5564114539\n",
      "  episode_reward_mean: -583798.362350118\n",
      "  episode_reward_min: -1790534.6667692396\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 3954\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2729345560073853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006628402508795261\n",
      "          model: {}\n",
      "          policy_loss: -0.004022764042019844\n",
      "          total_loss: 81067769856.0\n",
      "          vf_explained_var: 1.2243434866832104e-05\n",
      "          vf_loss: 81067769856.0\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.68\n",
      "    ram_util_percent: 17.1\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18161003829482503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08980964292377747\n",
      "    mean_inference_ms: 1.0954982413662706\n",
      "    mean_raw_obs_processing_ms: 0.13369515900274906\n",
      "  time_since_restore: 590.7849390506744\n",
      "  time_this_iter_s: 6.845173358917236\n",
      "  time_total_s: 590.7849390506744\n",
      "  timers:\n",
      "    learn_throughput: 1170.623\n",
      "    learn_time_ms: 3416.984\n",
      "    load_throughput: 12979433.7\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 596.52\n",
      "    sample_time_ms: 6705.56\n",
      "    update_time_ms: 2.61\n",
      "  timestamp: 1665667448\n",
      "  timesteps_since_restore: 356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:24:08,568\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 58380.0x the scale of `vf_clip_param`. This means that it will take more than 58380.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:24:10 (running for 00:10:03.09)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         590.785</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> -583798</td><td style=\"text-align: right;\">              364403</td><td style=\"text-align: right;\">        -1.79053e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_InventoryEnv_ea493_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-13_15-24-15\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 953603.4691257151\n",
      "  episode_reward_mean: -597572.9155973935\n",
      "  episode_reward_min: -1790534.6667692396\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 4000\n",
      "  experiment_id: 16c92e0843fe45b6a7fa3194faada972\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.708593726158142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2467707395553589\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007653938140720129\n",
      "          model: {}\n",
      "          policy_loss: -0.002141451695933938\n",
      "          total_loss: 79705309184.0\n",
      "          vf_explained_var: 7.646443009434734e-06\n",
      "          vf_loss: 79705309184.0\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.0.178\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.28888888888889\n",
      "    ram_util_percent: 17.166666666666668\n",
      "  pid: 27788\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18155421844233582\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08979819032119707\n",
      "    mean_inference_ms: 1.0956887842867447\n",
      "    mean_raw_obs_processing_ms: 0.1336506337058655\n",
      "  time_since_restore: 597.3437223434448\n",
      "  time_this_iter_s: 6.558783292770386\n",
      "  time_total_s: 597.3437223434448\n",
      "  timers:\n",
      "    learn_throughput: 1170.699\n",
      "    learn_time_ms: 3416.762\n",
      "    load_throughput: 13020734.187\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 598.017\n",
      "    sample_time_ms: 6688.775\n",
      "    update_time_ms: 2.61\n",
      "  timestamp: 1665667455\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: ea493_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=27788)\u001b[0m 2022-10-13 15:24:15,175\tWARNING ppo.py:162 -- The magnitude of your environment rewards are more than 59757.0x the scale of `vf_clip_param`. This means that it will take more than 59757.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-13 15:24:16 (running for 00:10:08.70)<br>Memory usage on this node: 10.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.04 GiB heap, 0.0/16.52 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/dibya/Dropbox/rl_course/real_world_deep_rl/11_using_rllib_to_solve_custom_gym_environment/experiment_vanilla/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_InventoryEnv_ea493_00000</td><td>RUNNING </td><td>192.168.0.178:27788</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         597.344</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\"> -597573</td><td style=\"text-align: right;\">              953603</td><td style=\"text-align: right;\">        -1.79053e+06</td><td style=\"text-align: right;\">                90</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": InventoryEnv,    # Instead of strings e.g. \"CartPole-v1\", we pass the custom env class\n",
    "                 \"evaluation_interval\": 1000,\n",
    "                 # Each episode uses different shop params. Need lots of samples to gauge agent's performance\n",
    "                 \"evaluation_num_episodes\": 10000    \n",
    "                 },\n",
    "         checkpoint_freq=1000,\n",
    "         local_dir=\"experiment_vanilla\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e14da-4cd1-47dc-918b-19becae5e4f4",
   "metadata": {},
   "source": [
    "# My agent does not learn anything!\n",
    "\n",
    "<img src=\"images/no_learning.png\" width=\"500\" />\n",
    "\n",
    "## In the next videos in the course, we will learn various tricks that improve performance in a wide range of custom environments.\n",
    "\n",
    "1. Observation Normalization\n",
    "2. Action Normalization\n",
    "3. Reward Scaling\n",
    "4. Simple Hyperparameter Tuning\n",
    "5. Advanced hyperparameter tuning e.g. Population Based Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43439e-b951-470c-9e33-7bd3fc3829bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
