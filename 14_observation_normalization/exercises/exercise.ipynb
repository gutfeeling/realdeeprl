{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a7c5bf-4127-44af-9643-b95914ba7c6c",
   "metadata": {},
   "source": [
    "# Using wrappers to derive `InventoryEnvHard` from `InventoryEnv`\n",
    "\n",
    "In the last chapter's exercises, we have implemented `InventoryEnvHard` from scratch. In this chapter's exercises, your job is to write wrappers so that we can derive `InventoryEnvHard` by wrapping `InventoryEnv`. We are aiming for\n",
    "\n",
    "```\n",
    "inventory_env_hard = Wrapper1(Wrapper2(...WrapperN(InventoryEnv())))...\n",
    "```\n",
    "\n",
    "instead of \n",
    "\n",
    "```\n",
    "inventory_env_hard = InventoryEnvHard()\n",
    "```\n",
    "\n",
    "This will drastically reduce code duplication and make the implementation clean and flexible.\n",
    "\n",
    "The `InventoryEnvHard` environment differs from `InventoryEnv` in terms of the state definition and reward function.\n",
    "\n",
    "In this exercise, your job is to write a wrapper which changes the state definition of `InventoryEnv` to include an additional element for `goodwill_penalty_per_unit`.\n",
    "\n",
    "I have included the `InventoryEnv` definition in the following code block. Please run it before starting this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab52c91-e6e5-4ad5-abf8-1b8a8850e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "class InventoryEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Must define self.observation_space and self.action_space here\n",
    "        \"\"\"\n",
    "        self.max_capacity = 4000\n",
    "\n",
    "        self.action_space = Box(low=np.array([0]), high=np.array([self.max_capacity]))\n",
    "\n",
    "        self.lead_time = 5\n",
    "        self.obs_dim = self.lead_time + 4\n",
    "\n",
    "        self.max_mean_daily_demand = 200\n",
    "        self.max_unit_selling_price = 100\n",
    "        self.max_daily_holding_cost_per_unit = 5\n",
    "\n",
    "        obs_low = np.zeros((self.obs_dim,))\n",
    "        obs_high = np.array([self.max_capacity for _ in range(self.lead_time)] +\n",
    "                            [self.max_mean_daily_demand, self.max_unit_selling_price,\n",
    "                             self.max_unit_selling_price, self.max_daily_holding_cost_per_unit\n",
    "                             ]\n",
    "                            )\n",
    "        self.observation_space = Box(low=obs_low, high=obs_high)\n",
    "\n",
    "        self.rng = default_rng()\n",
    "\n",
    "        self.current_obs = None\n",
    "        self.episode_length_in_days = 90\n",
    "        self.day_num = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Returns: the observation of the initial state\n",
    "        Reset the environment to initial state so that a new episode (independent of previous ones) may start\n",
    "        \"\"\"\n",
    "        mean_daily_demand = self.rng.uniform() * self.max_mean_daily_demand\n",
    "        selling_price = self.rng.uniform() * self.max_unit_selling_price\n",
    "        buying_price = self.rng.uniform() * selling_price\n",
    "        daily_holding_cost_per_unit = self.rng.uniform() * min(buying_price,\n",
    "                                                               self.max_daily_holding_cost_per_unit\n",
    "                                                               )\n",
    "        self.current_obs = np.array([0 for _ in range(self.lead_time)] +\n",
    "                                    [mean_daily_demand, selling_price, buying_price,\n",
    "                                     daily_holding_cost_per_unit,\n",
    "                                     ]\n",
    "                                    )\n",
    "        self.day_num = 0\n",
    "        return self.current_obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns: Given current obs and action, returns the next observation, the reward, done and optionally additional info\n",
    "        \"\"\"\n",
    "        buys = min(action[0], self.max_capacity - np.sum(self.current_obs[:self.lead_time]))\n",
    "\n",
    "        demand = self.rng.poisson(self.current_obs[self.lead_time])\n",
    "        next_obs = np.concatenate((self.current_obs[1: self.lead_time],\n",
    "                                   np.array([buys]),\n",
    "                                   self.current_obs[self.lead_time:]\n",
    "                                   )\n",
    "                                  )\n",
    "        next_obs[0] += max(0, self.current_obs[0] - demand)\n",
    "\n",
    "        reward = (self.current_obs[self.lead_time + 1] * (self.current_obs[0] + self.current_obs[1] - next_obs[0]) -\n",
    "                  self.current_obs[self.lead_time + 2] * buys -\n",
    "                  self.current_obs[self.lead_time + 3] * (next_obs[0] - self.current_obs[1])\n",
    "                  )\n",
    "\n",
    "        self.day_num += 1\n",
    "        done = False\n",
    "        if self.day_num >= self.episode_length_in_days:\n",
    "            done = True\n",
    "\n",
    "        self.current_obs = next_obs\n",
    "\n",
    "        return self.current_obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        Show the current environment state e.g. the graphical window in `CartPole-v1`\n",
    "        This method must be implemented, but it is OK to have an empty implementation if rendering is not\n",
    "        important\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        This method is optional. Used to cleanup all resources (threads, graphical windows) etc.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Returns: List of seeds\n",
    "        This method is optional. Used to set seeds for the environment's random number generator for\n",
    "        obtaining deterministic behavior\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a7347-c4c6-49dd-ac76-f132c09fbb55",
   "metadata": {},
   "source": [
    "Implement the wrapper in the following code block.\n",
    "\n",
    "HINTS:\n",
    "\n",
    "1. Which methods return the state? Perhaps you need to override some or all of them.\n",
    "2. Don't forget about the observation space.\n",
    "3. Can you use the convenience class `gym.ObservationWrapper` to simplify the implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac31d8-ab63-4ba5-ae5a-345dc544dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifyObservation(...):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef9020d-552e-46b8-b76e-2868a1c15a9b",
   "metadata": {},
   "source": [
    "Now test your wrapper. First, create a wrapped environment using the `ModifyObservation` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887494f5-b455-4318-b719-597c20514805",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d30bf-adaf-401b-8590-a8a79700af14",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "1. Reset your environment and obtain the initial state. Does the state have one more element compared to `InventoryEnv`? Is it in the expected range $0 \\le \\mathrm{goodwill\\_penalty\\_per\\_unit} \\le 10$? \n",
    "\n",
    "2. Reset the environment again. Do you get a different value for the last element?\n",
    "\n",
    "3. Perform a random action on the wrapped environment and obtain the next state. Does the new state look alright?\n",
    "\n",
    "4. Does `wrapped.observation_space` have 10 dimensions (instead of 9)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e18744a-863f-49c3-92d7-313115b6bdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
