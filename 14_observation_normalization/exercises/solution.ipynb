{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a16067d9-a5d8-4a3f-a999-9bafe5e03eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "class InventoryEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Must define self.observation_space and self.action_space here\n",
    "        \"\"\"\n",
    "        self.max_capacity = 4000\n",
    "\n",
    "        self.action_space = Box(low=np.array([0]), high=np.array([self.max_capacity]))\n",
    "\n",
    "        self.lead_time = 5\n",
    "        self.obs_dim = self.lead_time + 4\n",
    "\n",
    "        self.max_mean_daily_demand = 200\n",
    "        self.max_unit_selling_price = 100\n",
    "        self.max_daily_holding_cost_per_unit = 5\n",
    "\n",
    "        obs_low = np.zeros((self.obs_dim,))\n",
    "        obs_high = np.array([self.max_capacity for _ in range(self.lead_time)] +\n",
    "                            [self.max_mean_daily_demand, self.max_unit_selling_price,\n",
    "                             self.max_unit_selling_price, self.max_daily_holding_cost_per_unit\n",
    "                             ]\n",
    "                            )\n",
    "        self.observation_space = Box(low=obs_low, high=obs_high)\n",
    "\n",
    "        self.rng = default_rng()\n",
    "\n",
    "        self.current_obs = None\n",
    "        self.episode_length_in_days = 90\n",
    "        self.day_num = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Returns: the observation of the initial state\n",
    "        Reset the environment to initial state so that a new episode (independent of previous ones) may start\n",
    "        \"\"\"\n",
    "        mean_daily_demand = self.rng.uniform() * self.max_mean_daily_demand\n",
    "        selling_price = self.rng.uniform() * self.max_unit_selling_price\n",
    "        buying_price = self.rng.uniform() * selling_price\n",
    "        daily_holding_cost_per_unit = self.rng.uniform() * min(buying_price,\n",
    "                                                               self.max_daily_holding_cost_per_unit\n",
    "                                                               )\n",
    "        self.current_obs = np.array([0 for _ in range(self.lead_time)] +\n",
    "                                    [mean_daily_demand, selling_price, buying_price,\n",
    "                                     daily_holding_cost_per_unit,\n",
    "                                     ]\n",
    "                                    )\n",
    "        self.day_num = 0\n",
    "        return self.current_obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns: Given current obs and action, returns the next observation, the reward, done and optionally additional info\n",
    "        \"\"\"\n",
    "        buys = min(action[0], self.max_capacity - np.sum(self.current_obs[:self.lead_time]))\n",
    "\n",
    "        demand = self.rng.poisson(self.current_obs[self.lead_time])\n",
    "        next_obs = np.concatenate((self.current_obs[1: self.lead_time],\n",
    "                                   np.array([buys]),\n",
    "                                   self.current_obs[self.lead_time:]\n",
    "                                   )\n",
    "                                  )\n",
    "        next_obs[0] += max(0, self.current_obs[0] - demand)\n",
    "\n",
    "        reward = (self.current_obs[self.lead_time + 1] * (self.current_obs[0] + self.current_obs[1] - next_obs[0]) -\n",
    "                  self.current_obs[self.lead_time + 2] * buys -\n",
    "                  self.current_obs[self.lead_time + 3] * (next_obs[0] - self.current_obs[1])\n",
    "                  )\n",
    "\n",
    "        self.day_num += 1\n",
    "        done = False\n",
    "        if self.day_num >= self.episode_length_in_days:\n",
    "            done = True\n",
    "\n",
    "        self.current_obs = next_obs\n",
    "\n",
    "        return self.current_obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        Show the current environment state e.g. the graphical window in `CartPole-v1`\n",
    "        This method must be implemented, but it is OK to have an empty implementation if rendering is not\n",
    "        important\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        This method is optional. Used to cleanup all resources (threads, graphical windows) etc.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Returns: List of seeds\n",
    "        This method is optional. Used to set seeds for the environment's random number generator for\n",
    "        obtaining deterministic behavior\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab666fed-13c5-446b-b366-3e56d8f93800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeHard(gym.Wrapper):\n",
    "    # ----- SOLUTION ----- #\n",
    "    # Adjust observation space and obs_dim to match InventoryEnvHard\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.max_goodwill_penalty_per_unit = 10\n",
    "        obs_low = self.env.observation_space.low\n",
    "        obs_high = self.env.observation_space.high\n",
    "        self.observation_space = Box(\n",
    "            low = np.append(obs_low, 0),\n",
    "            high = np.append(obs_high, self.max_goodwill_penalty_per_unit)\n",
    "        )\n",
    "        self.obs_dim = self.env.obs_dim + 1\n",
    "        \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        # ----- SOLUTION ----- #\n",
    "        # For each episode, we need a new random value for goodwill_penalty_per_unit\n",
    "        # This value should be stored as an instance variable, so that other methods can use it while an episode is running\n",
    "        self.goodwill_penalty_per_unit = self.env.rng.uniform() * self.max_goodwill_penalty_per_unit\n",
    "        # ----- SOLUTION ----- #\n",
    "        # The state in InventoryEnvHard has one additional element compared to InventoryEnv\n",
    "        # It's the goodwill_penalty_per_unit\n",
    "        return np.append(obs, self.goodwill_penalty_per_unit)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, r, done, info = self.env.step(action)\n",
    "        # ----- SOLUTION ----- #\n",
    "        # The state in InventoryEnvHard has one additional element compared to InventoryEnv\n",
    "        # It's the goodwill_penalty_per_unit\n",
    "        return np.append(obs, self.goodwill_penalty_per_unit), r, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0037499f-3259-4205-bc41-6be50652937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InventoryEnv()\n",
    "wrapped = MakeHard(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ae4ba-f5cd-4c6d-bbea-a2dd28344800",
   "metadata": {},
   "source": [
    "Notice how the state has one more element compared to the original env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57bd5b80-5655-465f-8de4-42ccbc3705ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       13.95544291, 77.01593137, 71.21781838,  3.27505571,  8.66245348])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped.reset()    # wrapped env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aab8e3d-2ef3-4594-b75f-1d20165a69bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        , 103.11751729,  26.2935599 ,   3.91117194,\n",
       "         2.33109171])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()    # original env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0911c-665e-4f2c-a5f0-b670b39a5df5",
   "metadata": {},
   "source": [
    "We get different values of `goodwill_penalty_per_unit` for each episode, as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f768cf6-cf5b-49a0-859b-d23fe9c0fb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        , 194.79944821,  72.0614243 ,  63.9324476 ,\n",
       "         0.79374726,   7.63863415])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped.reset()    # last element has a new random value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7d3aa-aa9b-4c9b-90af-00a3565b9204",
   "metadata": {},
   "source": [
    "The next state preserves the value of `goodwill_penalty_per_unit`, as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2268e611-9318-4cec-a6ee-72738c44588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.02853955e+03 1.94799448e+02 7.20614243e+01 6.39324476e+01\n",
      " 7.93747261e-01 7.63863415e+00]\n"
     ]
    }
   ],
   "source": [
    "obs, _, _, _ = wrapped.step(wrapped.action_space.sample())\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3d87a-9f56-4ca9-9301-2e6db056955a",
   "metadata": {},
   "source": [
    "Notice how the observation space of the wrapped environment has one additional dimension compared to the original environment. If you don't define this correctly in the wrapped environment, `rllib` will complain since it checks whether the observations belong to the observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bd7f8e4-9374-49a9-bcfe-d355fcbfedd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [4000. 4000. 4000. 4000. 4000.  200.  100.  100.    5.   10.], (10,), float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08ec4e2f-17d3-4a09-a886-90c4e4b34578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([0. 0. 0. 0. 0. 0. 0. 0. 0.], [4000. 4000. 4000. 4000. 4000.  200.  100.  100.    5.], (9,), float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
