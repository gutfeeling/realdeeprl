{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b59954-982e-4e67-90cb-ed6678dfd890",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "1. Modify the environment so that observations have standard range $\\left( -1, 1 \\right)$, $\\left( 0, 1 \\right)$ etc.\n",
    "2. Modify the environment so that actions have standard range $\\left( -1, 1 \\right)$, $\\left( 0, 1 \\right)$ etc.\n",
    "\n",
    "## RL algos work better/faster when rewards are non-sparse and have low variance\n",
    "\n",
    "| Variable | Good variance | Acceptable variance | Bad |\n",
    "| --- | --- | --- | --- |\n",
    "| rewards | 1 | 10 | 1000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6fd3b-0547-417c-871d-c6a2a8fb5411",
   "metadata": {},
   "source": [
    "## Option 1: Write a `gym` wrapper\n",
    "\n",
    "1. Set the wrapper's action space to `Box(low=np.array([-1,]), high=np.array([1,]))` $\\rightarrow$ the wrapped environment will accept actions in this standard range.\n",
    "2. In the wrapper's `step()` method, map the action in the standard range to the original environment's action space.\n",
    "\n",
    "<img src=\"images/action_norm/1.png\" width=\"500\"/>\n",
    "\n",
    "3. Use the mapped action to step through the original environment.\n",
    "\n",
    "## Option 2: Use `gym`'s built-in `RescaleAction` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db55d4e8-01cf-4a35-8a02-8987a0497c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RescaleAction\n",
    "import numpy as np\n",
    "\n",
    "from inventory_env.inventory_env import InventoryEnv\n",
    "\n",
    "normalized_action_env = RescaleAction(InventoryEnv(), min_action=np.array([-1]), max_action=np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea43f8-46d5-4653-bc2d-36fba26ab884",
   "metadata": {},
   "source": [
    "`gym` has a lot of useful built-in wrappers. Check them out here:\n",
    "\n",
    "- https://github.com/openai/gym/tree/v0.21.0/gym/wrappers\n",
    "- https://www.gymlibrary.dev/api/wrappers/ (careful, this corresponds to the latest version of `gym`, not the one that we are using)\n",
    "\n",
    "## You can chain wrappers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799bbcd7-20f9-4850-a8f5-b142f006e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inventory_env.wrappers import MyNormalizeObservation\n",
    "\n",
    "norm_action_obs_env = RescaleAction(MyNormalizeObservation(InventoryEnv()), min_action=np.array([-1]), max_action=np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4002f1a-c4c2-4781-990a-4402e77d2fe0",
   "metadata": {},
   "source": [
    "## Option 3: `rllib` implements action normalization by default\n",
    "\n",
    "- `\"normalize_action\"` key in [common algorithm configuration](https://docs.ray.io/en/releases-1.11.1/rllib/rllib-training.html#common-parameters) is set to `True` by default\n",
    "\n",
    "<img src=\"images/action_norm/2.png\" width=\"700\"/>\n",
    "\n",
    "- When using `rllib` for learning, we don't need to use any `gym` wrappers for action normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
