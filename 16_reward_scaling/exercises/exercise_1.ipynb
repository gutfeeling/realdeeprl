{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1746af20-4a13-4d85-8a61-e5e2e31bdcce",
   "metadata": {},
   "source": [
    "# Write a reward wrapper to apply penalty for unfulfilled demand (goodwill penalty)\n",
    "\n",
    "In the last exercise, we wrote a `ModifyObservation` wrapper that modified the observation of `InventoryEnv` to return the state required by `InventoryEnvHard`. \n",
    "\n",
    "`InventoryEnvHard` also uses a different reward function which penalizes unfulfilled demand. In particular, we need to add the following term to the reward of `InventoryEnv`.\n",
    "\n",
    "$$-k \\max(0, d - I)$$.\n",
    "\n",
    "Here, $d$ is the realized demand for the day (sampled from a Poisson distribution), $I$ is the on-hand-inventory, and $k$ is the `goodwill_penalty_per_unit`.\n",
    "\n",
    "In this exercise, your job is to write a reward wrapper `ModifyReward` so that we can instantiate the hard environment as follows\n",
    "\n",
    "```\n",
    "inventory_env_hard = ModifyReward(ModifyState(env=InventoryEnv()))\n",
    "```\n",
    "\n",
    "Notice that the additional reward term depends on demand. However, in the `step()` method of `InventoryEnv`, the demand (after we randomly generate it from a Poisson distribution) is neither stored as an instance variable nor returned in the `info` dict.  But the wrapper must have access to the demand to compute the additional term. \n",
    "\n",
    "To fix this flaw, I have included a `InventoryEnv` implementation below which is slightly different from the lesson. The only difference is that the `demand` is returned in the `info` dict. Please use the class below when writing your wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94aaf060-6d2f-4514-81ef-3cd37c060555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "class InventoryEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Must define self.observation_space and self.action_space here\n",
    "        \"\"\"\n",
    "        self.max_capacity = 4000\n",
    "\n",
    "        self.action_space = Box(low=np.array([0]), high=np.array([self.max_capacity]))\n",
    "\n",
    "        self.lead_time = 5\n",
    "        self.obs_dim = self.lead_time + 4\n",
    "\n",
    "        self.max_mean_daily_demand = 200\n",
    "        self.max_unit_selling_price = 100\n",
    "        self.max_daily_holding_cost_per_unit = 5\n",
    "\n",
    "        obs_low = np.zeros((self.obs_dim,))\n",
    "        obs_high = np.array([self.max_capacity for _ in range(self.lead_time)] +\n",
    "                            [self.max_mean_daily_demand, self.max_unit_selling_price,\n",
    "                             self.max_unit_selling_price, self.max_daily_holding_cost_per_unit\n",
    "                             ]\n",
    "                            )\n",
    "        self.observation_space = Box(low=obs_low, high=obs_high)\n",
    "\n",
    "        self.rng = default_rng()\n",
    "\n",
    "        self.current_obs = None\n",
    "        self.episode_length_in_days = 90\n",
    "        self.day_num = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Returns: the observation of the initial state\n",
    "        Reset the environment to initial state so that a new episode (independent of previous ones) may start\n",
    "        \"\"\"\n",
    "        mean_daily_demand = self.rng.uniform() * self.max_mean_daily_demand\n",
    "        selling_price = self.rng.uniform() * self.max_unit_selling_price\n",
    "        buying_price = self.rng.uniform() * selling_price\n",
    "        daily_holding_cost_per_unit = self.rng.uniform() * min(buying_price,\n",
    "                                                               self.max_daily_holding_cost_per_unit\n",
    "                                                               )\n",
    "        self.current_obs = np.array([0 for _ in range(self.lead_time)] +\n",
    "                                    [mean_daily_demand, selling_price, buying_price,\n",
    "                                     daily_holding_cost_per_unit,\n",
    "                                     ]\n",
    "                                    )\n",
    "        self.day_num = 0\n",
    "        return self.current_obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns: Given current obs and action, returns the next observation, the reward, done and optionally additional info\n",
    "        \"\"\"\n",
    "        buys = min(action[0], self.max_capacity - np.sum(self.current_obs[:self.lead_time]))\n",
    "\n",
    "        demand = self.rng.poisson(self.current_obs[self.lead_time])\n",
    "        next_obs = np.concatenate((self.current_obs[1: self.lead_time],\n",
    "                                   np.array([buys]),\n",
    "                                   self.current_obs[self.lead_time:]\n",
    "                                   )\n",
    "                                  )\n",
    "        next_obs[0] += max(0, self.current_obs[0] - demand)\n",
    "\n",
    "        reward = (self.current_obs[self.lead_time + 1] * (self.current_obs[0] + self.current_obs[1] - next_obs[0]) -\n",
    "                  self.current_obs[self.lead_time + 2] * buys -\n",
    "                  self.current_obs[self.lead_time + 3] * (next_obs[0] - self.current_obs[1])\n",
    "                  )\n",
    "\n",
    "        self.day_num += 1\n",
    "        done = False\n",
    "        if self.day_num >= self.episode_length_in_days:\n",
    "            done = True\n",
    "\n",
    "        self.current_obs = next_obs\n",
    "        \n",
    "        # ----- THIS IS DIFFERENT FROM THE VIDEO LESSONS ----- #\n",
    "        # We return the demand in the info dict so that any wrapper can access it\n",
    "        return self.current_obs, reward, done, {\"demand\": demand}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        Show the current environment state e.g. the graphical window in `CartPole-v1`\n",
    "        This method must be implemented, but it is OK to have an empty implementation if rendering is not\n",
    "        important\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Returns: None\n",
    "        This method is optional. Used to cleanup all resources (threads, graphical windows) etc.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Returns: List of seeds\n",
    "        This method is optional. Used to set seeds for the environment's random number generator for\n",
    "        obtaining deterministic behavior\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ed629-73c6-4dd3-9979-d6f114cee58f",
   "metadata": {},
   "source": [
    "Write your implementation of `ModifyReward` below.\n",
    "\n",
    "HINT: You can access any instance variable defined in the wrapped env (e.g. `self.current_obs`) inside the wrapper's methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25f48b-7b30-4931-9813-36ba371c63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifyReward(...):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83d4ee-107b-488c-a329-807ec793bd3e",
   "metadata": {},
   "source": [
    "Next, check that your `ModifyReward` wrapper works.\n",
    "\n",
    "1. First, create an environment instance by chaining the `ModifyReward` and `ModifyObservation` wrappers (I have supplied the correct implementation of `ModifyObservation` below).\n",
    "2. Run an episode and always take action 0. In the original `InventoryEnv`, this would lead to zero rewards (no buying costs, no profits, and no holding cost). But in the wrapped version of the environment, you should have negative rewards (because of unfulfilled demand). Verify that this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45b54af-84bc-43b1-ae69-82f3d9148531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import ObservationWrapper\n",
    "\n",
    "\n",
    "class ModifyObservation(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.max_goodwill_penalty_per_unit = 10\n",
    "        obs_low = self.env.observation_space.low\n",
    "        obs_high = self.env.observation_space.high\n",
    "        self.observation_space = Box(\n",
    "            low = np.append(obs_low, 0),\n",
    "            high = np.append(obs_high, self.max_goodwill_penalty_per_unit)\n",
    "        )\n",
    "        \n",
    "    def reset(self):\n",
    "        self.goodwill_penalty_per_unit = self.env.rng.uniform() * self.max_goodwill_penalty_per_unit\n",
    "        return super().reset()\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        return np.append(obs, self.goodwill_penalty_per_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d89f598-ccd7-4ff5-9e88-55fb1c04d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment using the wrappers\n",
    "inventory_env_hard = ...\n",
    "\n",
    "# Run an episode, always taking action 0 (no buys). Check that the stepwise reward is negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
