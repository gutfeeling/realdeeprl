{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110e7218-2beb-4476-a75e-931173ea021e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparing best agent's performance with classical baseline\n",
    "\n",
    "### Three Steps\n",
    "\n",
    "1. Identify the checkpoint with best evaluation performance.\n",
    "2. Restore the agent with identified checkpoint.\n",
    "3. Run the restored agent on 100000 episodes with the same seed used for computing baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0ff5d-c0f1-4066-9f92-99651f7cf360",
   "metadata": {},
   "source": [
    "### Step 2: Restoring agent from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954b3f8e-6b0d-4290-941d-7db8407adc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 17:09:46,639\tWARNING deprecation.py:45 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1732)\u001b[0m /home/dibya/miniconda3/envs/real_world_deep_rl_course_cpu/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1732)\u001b[0m   logger.warn(\n",
      "2023-04-27 17:09:52,515\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "/home/dibya/miniconda3/envs/real_world_deep_rl_course_cpu/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "2023-04-27 17:09:53,174\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2023-04-27 17:09:53,237\tINFO trainable.py:495 -- Restored on 192.168.0.178 from checkpoint: /home/dibya/Dropbox/rl_course/real_world_deep_rl/inventory_management_rl/experiments/experiment_results/experiment_wrappers/PPO_inventory_env_b7bbd_00003_3_obs_filter=my_normalize,reward_filter=gym_scale_rewards,obs_filter=my_normalize_2023-03-30_14-31-15/checkpoint_003000/checkpoint-3000\n",
      "2023-04-27 17:09:53,238\tINFO trainable.py:503 -- Current state after restoring: {'_iteration': 3000, '_timesteps_total': 12000000, '_time_total': 32642.497244119644, '_episodes_total': 133333}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune import ExperimentAnalysis\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from inventory_env.env_creator import inventory_env_creator\n",
    "\n",
    "register_env(\"inventory_env\", inventory_env_creator)\n",
    "\n",
    "path_to_results_dir = Path().absolute().parent / \"inventory_management_rl\" / \"experiments\" / \"experiment_results\" / \"experiment_wrappers\"\n",
    "analysis = ExperimentAnalysis(path_to_results_dir, default_metric=\"evaluation/episode_reward_mean\", default_mode=\"max\")\n",
    "best_trial = analysis.get_best_trial(scope=\"all\")\n",
    "best_checkpoint = analysis.get_best_checkpoint(best_trial)\n",
    "config = best_trial.config\n",
    "agent = PPOTrainer(config=config)\n",
    "agent.restore(best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e58ea9-d260-4486-ac08-f41180951020",
   "metadata": {},
   "source": [
    "### Step 3: Run restored agent for 100000 episodes with seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35ba6a0-8704-4d3b-b39b-bef63c35c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:39:19,332\tWARNING deprecation.py:45 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175694.70564031467\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "env = inventory_env_creator(\n",
    "    {\n",
    "        \"obs_filter\": \"my_normalize\", \n",
    "        \"reward_filter\": None, \n",
    "    }, \n",
    "    seed=0\n",
    ")\n",
    "num_episodes = 100000\n",
    "all_r = []\n",
    "for _ in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        action = agent.compute_action(obs, unsquash_action=True)\n",
    "        obs, r, done, _ = env.step(np.around(action))\n",
    "        ep_r += r\n",
    "        if done:\n",
    "            break\n",
    "    all_r.append(ep_r)\n",
    "baseline = sum(all_r) / num_episodes\n",
    "print(baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
